{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJiJum1FC5HB",
        "outputId": "b1fe1555-dd0c-4b83-f190-1de234a4119d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vader-multi\n",
            "  Downloading vader_multi-3.2.2.1-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting translatte\n",
            "  Downloading translatte-0.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from vader-multi) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->vader-multi) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->vader-multi) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->vader-multi) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->vader-multi) (2.10)\n",
            "Installing collected packages: translatte, vader-multi\n",
            "Successfully installed translatte-0.1 vader-multi-3.2.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.8/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transvec\n",
            "  Downloading transvec-0.1.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transvec) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from transvec) (1.0.2)\n",
            "Requirement already satisfied: nltk>=3.5 in /usr/local/lib/python3.8/dist-packages (from transvec) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.5->transvec) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.5->transvec) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.5->transvec) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.5->transvec) (2022.6.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->transvec) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->transvec) (3.1.0)\n",
            "Installing collected packages: transvec\n",
            "Successfully installed transvec-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install numpy \n",
        "!pip install vader-multi\n",
        "!pip install gensim \n",
        "!pip install transvec "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocFab():\n",
        "    import pandas as pd\n",
        "    import json\n",
        "    import os\n",
        "\n",
        "    import numpy as np\n",
        "    import re #regex \n",
        "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "    import gensim\n",
        "    from transvec.transformers import TranslationWordVectorizer\n",
        "    import requests\n",
        "    import os\n",
        "    import zipfile\n",
        "\n",
        "    #cheat code until the code can be totally checked by Fabrice\n",
        "    df = pd.read_csv(\"clean_data_set_sentiment_only_for_english.csv\", low_memory = False)\n",
        "    return df\n",
        "\n",
        "    #extract the data from the json files\n",
        "    dfs = []\n",
        "    for r, d, f in os.walk('input'):\n",
        "        for file in f:\n",
        "            if 'withheldtweets.json' in file:  # alt: if ‘control' in file:\n",
        "                dfs.append(pd.read_json('%s/%s' % (r, file), lines=True))\n",
        "\n",
        "    df_cen = pd.concat(dfs)\n",
        "    df_cen = df_cen.dropna(subset=['withheld_in_countries'])\n",
        "\n",
        "\n",
        "    #keep only the features that are worth keeping\n",
        "    worthKeeping = [\"text\", \"truncated\", \"user\",\n",
        "                \"withheld_in_countries\", \"entities\", \"lang\",\n",
        "                \"possibly_sensitive\", \"extended_tweet\"]\n",
        "    df_cen = df_cen[worthKeeping]\n",
        "\n",
        "\n",
        "    #some tweets have NaN as \"possibly sensitive\"…\n",
        "    df_cen['possibly_sensitive'] = df_cen['possibly_sensitive'].fillna(0.0)\n",
        "\n",
        "\n",
        "    #recover the full text for truncated tweets\n",
        "    dfRaw = df_cen.values\n",
        "    for line in dfRaw:\n",
        "        if not pd.isna(line[-1]):\n",
        "            line[0] = line[-1][\"full_text\"]\n",
        "            \n",
        "        #remove urls from tweets\n",
        "        #they are shortened anyway so we can't make use of them\n",
        "        line[0] = re.sub(r'http\\S+', '', line[0])\n",
        "        \n",
        "        #flatten retweets\n",
        "        line[0] = re.sub(r'RT @\\S+:', '', line[0])\n",
        "\n",
        "    dfRaw = np.delete(dfRaw, len(worthKeeping)-1, axis=1) #remove \"extended_tweet\"\n",
        "    worthKeeping.remove(\"extended_tweet\")\n",
        "\n",
        "    dfRaw = np.delete(dfRaw, 1, axis=1) #remove \"truncated\"\n",
        "    worthKeeping.remove(\"truncated\")\n",
        "\n",
        "\n",
        "    #extract hashtags seperately\n",
        "    for line in dfRaw:\n",
        "        line[3] = [x[\"text\"] for x in line[3][\"hashtags\"]]\n",
        "    worthKeeping[3] = \"hashtags\"\n",
        "\n",
        "\n",
        "    #create a feature for user-verified and user-followers_count\n",
        "    verified = [line[1][\"verified\"] for line in dfRaw]\n",
        "    followers = [line[1][\"followers_count\"] for line in dfRaw]\n",
        "\n",
        "\n",
        "    #for the location, Rebekah suggested to only spot the country name and discard the rest\n",
        "    listOfCountries = ['Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia, Plurinational State of', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \"Korea, Democratic People's Republic of\", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe']\n",
        "    def findCountry(x): #the code for this function is self explanatory\n",
        "        for country in listOfCountries:\n",
        "            if x and country in x:\n",
        "                return country\n",
        "        return None\n",
        "\n",
        "    location = [findCountry(line[1][\"location\"]) for line in dfRaw]\n",
        "\n",
        "    dfRaw = np.c_[dfRaw, verified, followers, location]\n",
        "    worthKeeping += [\"verified_account\", \"followers_count\", \"location\"]\n",
        "\n",
        "\n",
        "    #binary feature for whether the tweet has been withheld anywhere\n",
        "    withheld = []\n",
        "    for line in dfRaw:\n",
        "        if not isinstance(line[2], list):\n",
        "            line[2] = []\n",
        "        withheld.append(len(line[2]) != 0)\n",
        "            \n",
        "    dfRaw = np.c_[dfRaw, withheld]\n",
        "    worthKeeping += [\"withheld_anywhere\"]\n",
        "\n",
        "\n",
        "    #sentiment analysis\n",
        "    #https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/? with VADER\n",
        "    #https://github.com/brunneis/vader-multi, same concept but multilingual\n",
        "    #text gets translated into english and then sentiment analysis is applied to the english text\n",
        "    #takes a LOT of time\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    #we made the assumption that sentiment analysis for this analyzer only works for english\n",
        "    res = np.array([[x for x in analyzer.polarity_scores(line[0]).values()] for line in dfRaw])\n",
        "\n",
        "    dfRaw = np.c_[dfRaw, res]\n",
        "    worthKeeping += [\"neg\", \"neu\", \"pos\", \"compound\"]\n",
        "\n",
        "\n",
        "    #popularity feature:\n",
        "    #build a score based on the values of followers_count, favourites_count, statuses_count\n",
        "    #compute a score from 0 to 1 for each, with (x - min)/(max - min), then comptute the average of these scores \n",
        "\n",
        "    followers_count = np.array([line[1][\"followers_count\"] for line in dfRaw])\n",
        "    favourites_count = np.array([line[1][\"favourites_count\"] for line in dfRaw])\n",
        "    statuses_count = np.array([line[1][\"statuses_count\"] for line in dfRaw])\n",
        "\n",
        "    def normalize(array):\n",
        "        return (array - np.min(array)) / (np.max(array) - np.min(array))\n",
        "\n",
        "    score = (1/3) * (normalize(followers_count) + normalize(favourites_count) + normalize(statuses_count))\n",
        "    dfRaw = np.c_[dfRaw, score]\n",
        "    worthKeeping += [\"popularity_score\"]\n",
        "\n",
        "\n",
        "    #reassemble the data in a pandas dataframe and remove the column \"user\"\n",
        "    df_cen = pd.DataFrame(dfRaw, columns = worthKeeping)\n",
        "    cleanCols = filter(lambda x: x != \"user\", worthKeeping)\n",
        "    df_clean = df_cen[cleanCols]\n",
        "\n",
        "\n",
        "    #word2vec\n",
        "    def download_model(url, filename, removeFile = True):\n",
        "        \"\"\"Download a word2vec model from the url\"\"\"\n",
        "        #download the model\n",
        "        if not os.path.exists(f\"input/{filename}\"):\n",
        "            with open(\"input/model-temp.zip\", \"wb\") as f: #download\n",
        "                f.write(requests.get(url).content)\n",
        "                \n",
        "            with zipfile.ZipFile(\"input/model-temp.zip\") as zf: #extract the data for the model\n",
        "                zf.extract(\"model.bin\", \"./input/\")\n",
        "\n",
        "            os.rename(\"input/model.bin\", f\"input/{filename}\")\n",
        "            os.remove(\"input/model-temp.zip\")\n",
        "\n",
        "        #make it\n",
        "        model = gensim.models.KeyedVectors.load_word2vec_format(f\"input/{filename}\", binary=True)\n",
        "        \n",
        "        if removeFile:\n",
        "            os.remove(f\"input/{filename}\")\n",
        "        \n",
        "        return model\n",
        "\n",
        "    def sentence2vec(sentence, model, split=True):\n",
        "        \"\"\"Compute the word2vec vector of the text of a tweet\"\"\"\n",
        "        vector = None\n",
        "        n = 0\n",
        "        \n",
        "        def removePunctNumbers(sentence):\n",
        "            toRemove = \"²&~\\\"#'{}'([])|-`\\_^@+1234567890°$£µ*%!§:/;.,?<>€\"\n",
        "            for char in toRemove: sentence = sentence.replace(char, \"\")\n",
        "            return sentence.lower()\n",
        "        \n",
        "        words = removePunctNumbers(sentence)\n",
        "        if split: words = words.split(\" \")\n",
        "        else: words = list(words)\n",
        "\n",
        "        for word in words:\n",
        "            if word in model:\n",
        "                if vector is None: vector = model[word]\n",
        "                else: vector = vector + model[word]\n",
        "                n += 1\n",
        "\n",
        "        if n > 0:\n",
        "            return vector / n\n",
        "        else:\n",
        "            return np.zeros(300, dtype=np.float32)\n",
        "\n",
        "    #from http://vectors.nlpl.eu/repository/\n",
        "    models = { #urls of the models in the repository\n",
        "        \"en\": \"http://vectors.nlpl.eu/repository/20/8.zip\",\n",
        "        \"ru\": \"http://vectors.nlpl.eu/repository/20/184.zip\",\n",
        "        \"fr\": \"http://vectors.nlpl.eu/repository/20/43.zip\",\n",
        "        \"ur\": \"http://vectors.nlpl.eu/repository/20/72.zip\",\n",
        "        \"es\": \"http://vectors.nlpl.eu/repository/20/68.zip\",\n",
        "        \"ar\": \"http://vectors.nlpl.eu/repository/20/31.zip\",\n",
        "        \"in\": \"http://vectors.nlpl.eu/repository/20/50.zip\",\n",
        "        \"ja\": \"http://vectors.nlpl.eu/repository/20/53.zip\",\n",
        "        \"ko\": \"http://vectors.nlpl.eu/repository/20/55.zip\",\n",
        "        \"pt\": \"http://vectors.nlpl.eu/repository/20/63.zip\",\n",
        "        #there is no model for thai in this repository\n",
        "        \"tr\": \"http://vectors.nlpl.eu/repository/20/70.zip\",\n",
        "        #und is probably \"undetermined language\"\n",
        "    }\n",
        "    referrence_words = [\"water\", \"sun\", \"sky\", \"king\", \"night\", \"year\"]\n",
        "    #referrence words are used to \"rotate\" the vector spaces of different\n",
        "    #languages so that they become comparable to the one of the model for\n",
        "    #english: we give those words and their translations so that the translation\n",
        "    #has about the same vector as its english equivalent\n",
        "    ref_trans = {\n",
        "        \"ru\": ['вода', 'солнце', 'небо', 'король', 'сутки', 'год'],\n",
        "        \"fr\": [\"eau\", \"soleil\", \"ciel\", \"roi\", \"nuit\", \"année\"],\n",
        "        \"ur\": ['پانی', 'سورج', 'آسمان', 'بادشاہ', 'رات', 'سال'], #it looks reversed but it's not\n",
        "        \"es\": ['agua', 'dom', 'cielo', 'rey', 'noche', 'año'],\n",
        "        \"ar\": [\"ماء\", \"الشمس\", \"سماء\", \"ملِك\", \"ليل\", \"عام\"], #it looks reversed but it's not\n",
        "        \"in\": ['panas', 'matahari', 'langit', 'raja', 'malam', 'tahun'],\n",
        "        \"ja\": ['水', '日', '空', '王', '泊', '年'], \n",
        "        \"ko\": ['물', '일', '하늘', '왕', '박', '년'],\n",
        "        \"pt\": ['água', 'eno', 'céu', 'cama', 'noite', 'ano'],\n",
        "        \"tr\": ['su', 'güneş', 'gökyüzü', 'kral', 'gece', 'yıl']\n",
        "    }\n",
        "\n",
        "\n",
        "    #get the model for english\n",
        "    en_model = download_model(models[\"en\"], \"model_en.bin\", removeFile=False)\n",
        "\n",
        "    #get the raw numpy array of the data and add 300 new features: the components of word2vec vectors\n",
        "    data = df.values\n",
        "    columns = list(df.columns)\n",
        "    del df\n",
        "    data = np.c_[data, np.zeros((data.shape[0], 300))]\n",
        "\n",
        "    #apply sentence2vec to tweets in a certain lang\n",
        "    def vectorizeLang(lang, model, data):\n",
        "        for i, line in enumerate(data):\n",
        "            text, langLine = line[0], line[3]\n",
        "            if langLine == lang:\n",
        "                data[i][-300:] = sentence2vec(text, model, lang != \"ja\")\n",
        "                #japanese is different since vectors are defined for one symbol\n",
        "                #rather than one set of characters separated with a space, like\n",
        "                #for languages using the latin alphabet\n",
        "                #so the tokenization is different\n",
        "    \n",
        "    #once it's done, it's time to do it for other languages\n",
        "    #since the models aren't in the same vector space, we use TranslationWordVectorizer\n",
        "    for lang in ref_trans:\n",
        "        if lang not in models or \"ru\" == lang: continue\n",
        "        print(lang)\n",
        "        #download the model\n",
        "        lang_model = download_model(models[lang], f\"model_{lang}.bin\", False)\n",
        "        en_model = download_model(models[\"en\"], \"model_en.bin\", False)\n",
        "        print(\"ok\")\n",
        "        \n",
        "        #time to compute the bilingual model -> model for lang that is in the same vector space\n",
        "        #as the model of english\n",
        "        train_ref = [(a, b) for a, b in zip(referrence_words, ref_trans[lang])]\n",
        "        bilingual_model = TranslationWordVectorizer(en_model, lang_model).fit(train_ref)\n",
        "        print(\"bilingual ok\")\n",
        "        \n",
        "        #let's delete lang_model, it's no longer useful and takes a lot of memory\n",
        "        del lang_model\n",
        "        \n",
        "        vectorizeLang(lang, bilingual_model, data)\n",
        "        print(\"lang completed ---\")\n",
        "        \n",
        "        del bilingual_model\n",
        "    \n",
        "    for lang in ['ru']: #russian is different, the type of words is included in the model\n",
        "        print(lang)\n",
        "        #download the model\n",
        "        lang_model = download_model(models[lang], f\"model_{lang}.bin\", False)\n",
        "        en_model = download_model(models[\"en\"], \"model_en.bin\", False)\n",
        "        lang_model.vocab = {k.split(\"_\")[0]: v for k, v in lang_model.vocab.items()}\n",
        "        print(\"ok\")\n",
        "        \n",
        "        #time to compute the bilingual model -> model for lang that is in the same vector space\n",
        "        #as the model of english\n",
        "        train_ref = [(a, b) for a, b in zip(referrence_words, ref_trans[lang])]\n",
        "        bilingual_model = TranslationWordVectorizer(en_model, lang_model).fit(train_ref)\n",
        "        print(\"bilingual ok\")\n",
        "        \n",
        "        #let's delete lang_model, it's no longer useful and takes a lot of memory\n",
        "        del lang_model\n",
        "        \n",
        "        vectorizeLang(lang, bilingual_model, data)\n",
        "        print(\"lang completed ---\")\n",
        "\n",
        "        del bilingual_model\n",
        "\n",
        "    #make the word2vec vectors be a single column in the dataframe\n",
        "    dataExceptVector, vector = data[:,:-300], data[:,-300:]\n",
        "    vectors = list(vector[i] for i in range(vector.shape[0]))\n",
        "    df = pd.DataFrame(dataExceptVector, columns=columns[:-300])\n",
        "    df[\"vector\"] = vectors\n",
        "\n",
        "    df_clean.to_csv(\"clean_dataset.csv\")\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "Slv1KT1VEfcD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
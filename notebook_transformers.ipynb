{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downloads"
      ],
      "metadata": {
        "id": "3Qr0M2SzHS4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install preprocessor\n",
        "!pip install vaderSentiment\n",
        "!pip install torchmetrics\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5S8X4PutBIf",
        "outputId": "4ceba838-9a97-415b-a23e-61ce35ea855e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: preprocessor in /usr/local/lib/python3.8/dist-packages (1.1.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.8/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (5.5.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly) (8.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "bO98axTwHWr4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tvPM-LXFsp30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286ba8b8-77a6-4778-d394-454d70dd281e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import preprocessor as p\n",
        "import re\n",
        "import numpy as np\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
        "from transformers import ElectraConfig, ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
        "\n",
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "w6awx83KJNZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = 'df_english_clean.csv'\n",
        "seed_val = 42\n",
        "MODEL = 'ROBERTA' #'BERT', 'ELECTRA', 'ROBERTA'"
      ],
      "metadata": {
        "id": "RzUwBqzJJPBt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Import \n",
        "See README for Instructions"
      ],
      "metadata": {
        "id": "d__2kjNBHskh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(dir_path)"
      ],
      "metadata": {
        "id": "yiqUkFGUs-6m"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "qFKwnDWGtM-y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b206a90-75a2-4497-e3c2-2b92b89372da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text withheld_in_countries  \\\n",
              "0                              Pak Army is our pride                ['IN']   \n",
              "1  Ben Dunk \"If you haven't seen Lahore, you have...                ['IN']   \n",
              "2  India has also bestowed Abhinandan Varthaman w...                ['IN']   \n",
              "3  After the great Companion Saad bin Muadh embra...                ['RU']   \n",
              "4  After the great Companion Saad bin Muadh embra...                ['RU']   \n",
              "5  Paying homage to one of the great pilots in th...                ['IN']   \n",
              "6  Paying homage to one of the great pilots in th...                ['IN']   \n",
              "7                              STRIP CHAT STRIP CHAT                ['DE']   \n",
              "8  Champions of Democracy are in fact champions o...                ['IN']   \n",
              "9  Militant commander Nooristan known as Hasan Ba...                ['IN']   \n",
              "\n",
              "                      hashtags lang  possibly_sensitive  verified_account  \\\n",
              "0                  ['Balakot']   en                 0.0             False   \n",
              "1                           []   en                 0.0             False   \n",
              "2  ['ہماراایمان_دفاع_پاکستان']   en                 0.0             False   \n",
              "3                           []   en                 0.0             False   \n",
              "4                           []   en                 0.0             False   \n",
              "5               ['Abhinandan']   en                 0.0             False   \n",
              "6               ['Abhinandan']   en                 0.0             False   \n",
              "7                           []   en                 1.0             False   \n",
              "8                           []   en                 0.0             False   \n",
              "9          ['SouthWaziristan']   en                 0.0             False   \n",
              "\n",
              "   followers_count              location              user_id  \\\n",
              "0               93                   NaN   966615015716458497   \n",
              "1             3525                   NaN  1022545022447759360   \n",
              "2             3746  United Arab Emirates  1178014718029651969   \n",
              "3              354                   NaN  1043725274280538112   \n",
              "4              354                   NaN  1043725274280538112   \n",
              "5              411              Pakistan  1182959113929740296   \n",
              "6              411              Pakistan  1182959113929740296   \n",
              "7           195344                   NaN            148782077   \n",
              "8             1961              Pakistan  1322144832987537409   \n",
              "9             1036              Pakistan  1270480467310055426   \n",
              "\n",
              "   withheld_anywhere   neg   neu   pos  compound  popularity_score  \n",
              "0                  1  0.00  0.79  0.21      0.34          5.42e-04  \n",
              "1                  1  0.00  1.00  0.00      0.00          2.05e-02  \n",
              "2                  1  0.00  0.79  0.21      0.54          1.38e-02  \n",
              "3                  1  0.10  0.76  0.14      0.32          1.41e-03  \n",
              "4                  1  0.10  0.76  0.14      0.32          1.41e-03  \n",
              "5                  1  0.05  0.55  0.40      0.93          7.90e-04  \n",
              "6                  1  0.05  0.55  0.40      0.93          7.90e-04  \n",
              "7                  1  0.00  1.00  0.00      0.00          2.29e-02  \n",
              "8                  1  0.11  0.66  0.23      0.57          1.45e-03  \n",
              "9                  1  0.00  0.88  0.12      0.32          3.92e-03  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8df3a770-b8d1-4f6c-8fb4-a45d1793cf9c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>withheld_in_countries</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>lang</th>\n",
              "      <th>possibly_sensitive</th>\n",
              "      <th>verified_account</th>\n",
              "      <th>followers_count</th>\n",
              "      <th>location</th>\n",
              "      <th>user_id</th>\n",
              "      <th>withheld_anywhere</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>compound</th>\n",
              "      <th>popularity_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pak Army is our pride</td>\n",
              "      <td>['IN']</td>\n",
              "      <td>['Balakot']</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>93</td>\n",
              "      <td>NaN</td>\n",
              "      <td>966615015716458497</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.34</td>\n",
              "      <td>5.42e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ben Dunk \"If you haven't seen Lahore, you have...</td>\n",
              "      <td>['IN']</td>\n",
              "      <td>[]</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>3525</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1022545022447759360</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.05e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>India has also bestowed Abhinandan Varthaman w...</td>\n",
              "      <td>['IN']</td>\n",
              "      <td>['ہماراایمان_دفاع_پاکستان']</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>3746</td>\n",
              "      <td>United Arab Emirates</td>\n",
              "      <td>1178014718029651969</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.54</td>\n",
              "      <td>1.38e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>After the great Companion Saad bin Muadh embra...</td>\n",
              "      <td>['RU']</td>\n",
              "      <td>[]</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>354</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1043725274280538112</td>\n",
              "      <td>1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.32</td>\n",
              "      <td>1.41e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>After the great Companion Saad bin Muadh embra...</td>\n",
              "      <td>['RU']</td>\n",
              "      <td>[]</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>354</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1043725274280538112</td>\n",
              "      <td>1</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.32</td>\n",
              "      <td>1.41e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Paying homage to one of the great pilots in th...</td>\n",
              "      <td>['IN']</td>\n",
              "      <td>['Abhinandan']</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>411</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>1182959113929740296</td>\n",
              "      <td>1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.93</td>\n",
              "      <td>7.90e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Paying homage to one of the great pilots in th...</td>\n",
              "      <td>['IN']</td>\n",
              "      <td>['Abhinandan']</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>411</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>1182959113929740296</td>\n",
              "      <td>1</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.93</td>\n",
              "      <td>7.90e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>STRIP CHAT STRIP CHAT</td>\n",
              "      <td>['DE']</td>\n",
              "      <td>[]</td>\n",
              "      <td>en</td>\n",
              "      <td>1.0</td>\n",
              "      <td>False</td>\n",
              "      <td>195344</td>\n",
              "      <td>NaN</td>\n",
              "      <td>148782077</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.29e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Champions of Democracy are in fact champions o...</td>\n",
              "      <td>['IN']</td>\n",
              "      <td>[]</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>1961</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>1322144832987537409</td>\n",
              "      <td>1</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.57</td>\n",
              "      <td>1.45e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Militant commander Nooristan known as Hasan Ba...</td>\n",
              "      <td>['IN']</td>\n",
              "      <td>['SouthWaziristan']</td>\n",
              "      <td>en</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>1036</td>\n",
              "      <td>Pakistan</td>\n",
              "      <td>1270480467310055426</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.32</td>\n",
              "      <td>3.92e-03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8df3a770-b8d1-4f6c-8fb4-a45d1793cf9c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8df3a770-b8d1-4f6c-8fb4-a45d1793cf9c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8df3a770-b8d1-4f6c-8fb4-a45d1793cf9c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing and Feature Encoding"
      ],
      "metadata": {
        "id": "aE8Lri5StfgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"withheld_anywhere\"] = df[\"withheld_anywhere\"].astype(int)\n",
        "labels = np.array(df[\"withheld_anywhere\"])"
      ],
      "metadata": {
        "id": "uzeY9_WRtdi8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def word_tokenize_wrapper(sentence): \n",
        "    if isinstance(sentence, str): \n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        tokens_without_sw = [word for word in words if not word in stopwords.words('english')]\n",
        "        limitized_words = [lemmatizer.lemmatize(word) for word in tokens_without_sw]\n",
        "        return tokens_without_sw\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "text_tokenized_list = list(df.apply(lambda x: word_tokenize_wrapper(x['text']), axis=1))\n",
        "cleaned_sentences = []\n",
        "for sentences in text_tokenized_list:\n",
        "    sentence = \" \".join(sentences)\n",
        "    cleaned_sentences.append(sentence)"
      ],
      "metadata": {
        "id": "fr0W6mQ1thel"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "3ttA6VSiuLam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "if MODEL == 'ROBERTA':\n",
        "  tokenizer =  BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "  model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = 2,  \n",
        "    output_attentions = False,\n",
        "    return_dict=False,\n",
        "    output_hidden_states = False)\n",
        "  \n",
        "elif MODEL == 'ELECTRA': \n",
        "  configuration = ElectraConfig()\n",
        "  model = ElectraModel(configuration)\n",
        "  configuration = model.config\n",
        "  tokenizer = ElectraTokenizer.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n",
        "  model = ElectraForSequenceClassification.from_pretrained(\"bhadresh-savani/electra-base-emotion\", \n",
        "    num_labels = 2,  \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False, \n",
        "    return_dict=False,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "else: \n",
        "  tokenizer =  BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "  model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 2,  \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False, \n",
        "    return_dict=False\n",
        ")\n",
        "  \n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "9qpvr-onKDI8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "2f76aa21-d2ef-4f25-e887-0217a1158af8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "max_length = 0\n",
        "for tweet in cleaned_sentences:\n",
        "    max_length = max(max_length, len(tweet))\n",
        "\n",
        "for sentence in cleaned_sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sentence,                  \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = max_length,       \n",
        "                        pad_to_max_length = True,\n",
        "                        truncation=True,\n",
        "                        return_attention_mask = True,  \n",
        "                        return_tensors = 'pt')\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print(input_ids.shape)\n",
        "print(attention_masks.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "wTVXHOv2ukw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62beaa3e-9e90-4d63-b7c8-a6685b77f93e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning:\n",
            "\n",
            "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25830, 156])\n",
            "torch.Size([25830, 156])\n",
            "torch.Size([25830])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyperparameters\n",
        "batch_size = 32\n",
        "optimizer = AdamW(model.parameters(),lr = 2e-5, eps = 1e-8)\n",
        "epochs = 6\n",
        "division = 0.8"
      ],
      "metadata": {
        "id": "sc4gpfAnJFbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dec668b-eff8-4316-a77b-83edb48a09bb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning:\n",
            "\n",
            "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "dataset_length = len(dataset)\n",
        "train_size = int(division * dataset_length)\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, (dataset_length - train_size)])\n",
        "train_dataloader = DataLoader(train_dataset,  sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
        "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "def keep_flat_labels(preds, labels): \n",
        "  y_pred.extend(np.argmax(preds, axis=1).flatten())\n",
        "  y_true.extend(labels.flatten())\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "VoWICbXmz4Lb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "training_stats = []\n",
        "start = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    tot_train_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        model.zero_grad()        \n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        tot_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = tot_train_loss / len(train_dataloader)            \n",
        "    print(\"\\n Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        keep_flat_labels(logits, label_ids)\n",
        "        \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Total Training and Validation Time: \" + str(round(time.time() - start, 2)))"
      ],
      "metadata": {
        "id": "oWte01IAwsDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc38dd72-21bf-42e2-8686-c081f1fa6b58"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 6 ========\n",
            "\n",
            " Average training loss: 0.59\n",
            "  Accuracy: 0.75\n",
            "  Validation Loss: 0.50\n",
            "======== Epoch 2 / 6 ========\n",
            "\n",
            " Average training loss: 0.48\n",
            "  Accuracy: 0.78\n",
            "  Validation Loss: 0.45\n",
            "======== Epoch 3 / 6 ========\n",
            "\n",
            " Average training loss: 0.39\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.46\n",
            "======== Epoch 4 / 6 ========\n",
            "\n",
            " Average training loss: 0.32\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.51\n",
            "======== Epoch 5 / 6 ========\n",
            "\n",
            " Average training loss: 0.27\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.55\n",
            "======== Epoch 6 / 6 ========\n",
            "\n",
            " Average training loss: 0.23\n",
            "  Accuracy: 0.82\n",
            "  Validation Loss: 0.54\n",
            "Total Training and Validation Time: 3096.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results Visualization"
      ],
      "metadata": {
        "id": "SBNlBLsCJhPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats).set_index('epoch')\n",
        "df_stats\n",
        "res = classification_report(y_true, y_pred, target_names=['Not Censord', 'Censord']) \n",
        "print(res)\n",
        "plt.plot(df_stats['Training Loss'], 'r-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'b-o', label=\"Validation\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoch Number\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xticks([1, 2, 3, 4, 5, 6])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d2kOvSkM0I77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "d99887b8-5e49-4bf2-9f7f-2966631e8984"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " Not Censord       0.75      0.71      0.73     12210\n",
            "     Censord       0.82      0.85      0.83     18786\n",
            "\n",
            "    accuracy                           0.80     30996\n",
            "   macro avg       0.79      0.78      0.78     30996\n",
            "weighted avg       0.79      0.80      0.79     30996\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
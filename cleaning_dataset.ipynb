{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a26c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re #regex \n",
    "import preprocessor as p\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for r, d, f in os.walk('input'):\n",
    "    for file in f:\n",
    "        if 'withheldtweets.json' in file or 'plus_one_control.json' in file:  # alt: if ‘control' in file:\n",
    "            dfs.append(pd.read_json('%s/%s' % (r, file), lines=True))\n",
    "\n",
    "df_cen = pd.concat(dfs)\n",
    "df_cen.drop_duplicates()\n",
    "#df_cen = df_cen.dropna(subset=['withheld_in_countries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb929ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "worthKeeping = [\"text\", \"truncated\", \"user\",\n",
    "                \"withheld_in_countries\", \"entities\", \"lang\",\n",
    "                \"possibly_sensitive\", \"extended_tweet\"]\n",
    "df_cen = df_cen[worthKeeping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the NaN with coherent values to make further processing easier\n",
    "df_cen['possibly_sensitive'] = df_cen['possibly_sensitive'].fillna(0.0)\n",
    "df_cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recover the full text for truncated tweets\n",
    "\n",
    "dfRaw = df_cen.values\n",
    "for line in dfRaw:\n",
    "    if not pd.isna(line[-1]):\n",
    "        line[0] = line[-1][\"full_text\"]\n",
    "        \n",
    "    #remove urls from tweets\n",
    "    #they are shortened anyway so we can't make use of them\n",
    "    line[0] = re.sub(r'http\\S+', '', line[0])\n",
    "    \n",
    "    #flatten retweets\n",
    "    line[0] = re.sub(r'RT @\\S+:', '', line[0])\n",
    "\n",
    "dfRaw = np.delete(dfRaw, len(worthKeeping)-1, axis=1) #remove \"extended_tweet\"\n",
    "worthKeeping.remove(\"extended_tweet\")\n",
    "\n",
    "dfRaw = np.delete(dfRaw, 1, axis=1) #remove \"truncated\"\n",
    "worthKeeping.remove(\"truncated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7647261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract hashtags seperately\n",
    "\n",
    "for line in dfRaw:\n",
    "    line[3] = [x[\"text\"] for x in line[3][\"hashtags\"]]\n",
    "worthKeeping[3] = \"hashtags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ebbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a feature for user-verified and user-followers_count\n",
    "\n",
    "print(dfRaw[0][1].keys())\n",
    "verified = [line[1][\"verified\"] for line in dfRaw]\n",
    "followers = [line[1][\"followers_count\"] for line in dfRaw]\n",
    "\n",
    "#for the location, Rebekah suggested to only spot the country name and discard the rest\n",
    "listOfCountries = ['Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia, Plurinational State of', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \"Korea, Democratic People's Republic of\", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe']\n",
    "def findCountry(x):\n",
    "    for country in listOfCountries:\n",
    "        if x and country in x:\n",
    "            return country\n",
    "    return None\n",
    "\n",
    "location = [findCountry(line[1][\"location\"]) for line in dfRaw]\n",
    "\n",
    "dfRaw = np.c_[dfRaw, verified, followers, location]\n",
    "worthKeeping += [\"verified_account\", \"followers_count\", \"location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c30480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary feature for whether the tweet has been withheld anywhere\n",
    "\n",
    "withheld = []\n",
    "for line in dfRaw:\n",
    "    if not isinstance(line[2], list):\n",
    "        line[2] = []\n",
    "    withheld.append(len(line[2]) != 0)\n",
    "        \n",
    "dfRaw = np.c_[dfRaw, withheld]\n",
    "worthKeeping += [\"withheld_anywhere\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis\n",
    "#https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/? with VADER\n",
    "\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "#we made the assumption that sentiment analysis for this analyzer only works for english\n",
    "res = np.array([[x for x in sentiment.polarity_scores(line[0]).values()] if line[4] == \"en\" else [0.0, 0.0, 0.0, 0.0] for line in dfRaw])\n",
    "\n",
    "dfRaw = np.c_[dfRaw, res]\n",
    "worthKeeping += [\"neg\", \"neu\", \"pos\", \"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity feature:\n",
    "#build a score based on the values of followers_count, favourites_count, statuses_count\n",
    "#compute a score from 0 to 1 for each, with (x - min)/(max - min), then comptute the average of these scores \n",
    "\n",
    "followers_count = np.array([line[1][\"followers_count\"] for line in dfRaw])\n",
    "favourites_count = np.array([line[1][\"favourites_count\"] for line in dfRaw])\n",
    "statuses_count = np.array([line[1][\"statuses_count\"] for line in dfRaw])\n",
    "\n",
    "def normalize(array):\n",
    "    return (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "\n",
    "score = (1/3) * (normalize(followers_count) + normalize(favourites_count) + normalize(statuses_count))\n",
    "dfRaw = np.c_[dfRaw, score]\n",
    "worthKeeping += [\"popularity_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reassemble the data in a pandas dataframe\n",
    "df_cen = pd.DataFrame(dfRaw, columns = worthKeeping)\n",
    "df_cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanCols = filter(lambda x: x != \"user\", worthKeeping)\n",
    "df_clean = df_cen[cleanCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"withheld_anywhere\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84592201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"neg\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"popularity_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cd881-6354-4295-a5ad-c66b98e4699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library: https://pypi.org/project/tweet-preprocessor/\n",
    "\n",
    "# p.clean(file{.JSON, .txt}) or p.tokenize(file{.JSON, .txt}) or p.parse(file{.JSON, .txt})\n",
    "\n",
    "# p.set_options(p.OPT.?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edfd875-b822-4e65-aba3-60825396cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option Name, Option Short Code:\n",
    "\n",
    "# URL, p.OPT.URL\n",
    "\n",
    "# Mention, p.OPT.MENTION\n",
    "\n",
    "# Hashtag, p.OPT.HASHTAG -> keeping for now\n",
    "\n",
    "# Reserved Words, p.OPT.RESERVED\n",
    "\n",
    "# Emoji, p.OPT.EMOJI -> keeping for now\n",
    "\n",
    "# Smiley, p.OPT.SMILEY -> keeping for now\n",
    "\n",
    "# Number, p.OPT.NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fbf83-f217-4610-9f31-f80a7bb6f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.NUMBER, p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED)\n",
    "df_clean[\"text\"] = df_clean.apply({\"text\": lambda line: p.clean(line)}) \n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079ecca-1433-453a-977c-076fa1459e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

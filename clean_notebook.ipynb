{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfa4051",
   "metadata": {},
   "source": [
    "## Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e38b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install preprocessor\n",
    "!pip install vader-multi\n",
    "!pip install torchmetrics\n",
    "!pip install sentence-transformers\n",
    "!pip install gensim\n",
    "!pip install requests\n",
    "!pip install transvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d4932",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7a17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import gensim\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import preprocessor as p\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transvec.transformers import TranslationWordVectorizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d7b3e",
   "metadata": {},
   "source": [
    "## Run Flags and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "76d1a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = date.today()\n",
    "\n",
    "random_seed = 42\n",
    "target_names = ['Not Censord', 'Censord']\n",
    "\n",
    "RUN_PREPROCESSING = True\n",
    "RUN_COUNTRY_DIVISION = True\n",
    "FIT_CORPUS_FEATURE_SPACE = True\n",
    "\n",
    "LANGUAGE_RUN = 'Japanese' #English, Spanish, Thai, Japanese\n",
    "FEATURE_SPACE = 'TFIDF' #BOW, TFIDF, Sentence2vec\n",
    "\n",
    "data_path = 'Data'\n",
    "results_path = 'Results'\n",
    "vector_path = 'Vectors'\n",
    "output_path = 'Output'\n",
    "procssed_data_path = 'Processed'\n",
    "\n",
    "language_run_dir = os.path.join(procssed_data_path, LANGUAGE_RUN)\n",
    "feature_space_dir = os.path.join(vector_path, FEATURE_SPACE)\n",
    "\n",
    "\n",
    "if not os.path.isdir(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "if not os.path.isdir(procssed_data_path):\n",
    "    os.makedirs(procssed_data_path)\n",
    "\n",
    "if not os.path.isdir(language_run_dir):\n",
    "    os.makedirs(language_run_dir)\n",
    "    \n",
    "if not os.path.isdir(results_path):\n",
    "    os.makedirs(results_path)\n",
    "    \n",
    "preprocessed_data_path = os.path.join(output_path, \"csv_processed.csv\")\n",
    "language_data_path = os.path.join(language_run_dir, \"csv_processed.csv\")\n",
    "feature_space_path = os.path.join(feature_space_dir, LANGUAGE_RUN)\n",
    "results_path = os.path.join(results_path, ('results_' + str(date_today) + '.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9572a",
   "metadata": {},
   "source": [
    " ## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3775bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfCountries = ['France', 'Turkey', 'Germany', 'India']\n",
    "def findCountry(x): \n",
    "    for country in listOfCountries:\n",
    "        if x and country in x:\n",
    "            return country\n",
    "    return None\n",
    "\n",
    "def normalize(array):\n",
    "        return (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "    \n",
    "def preprocess_data():\n",
    "    #extract the data from the json files\n",
    "    dfs = []\n",
    "    for r, d, f in os.walk('Data/'):\n",
    "        for file in f:\n",
    "            if 'withheldtweets.json' in file or \"plus_one_control.json\" in file:  # alt: if 'control' in file:\n",
    "                dfs.append(pd.read_json('%s/%s' % (r, file), lines=True))\n",
    "    df_cen = pd.concat(dfs)\n",
    "    \n",
    "    #keep only the features that are worth keeping\n",
    "    worthKeeping = [\"text\", \"truncated\", \"user\",\n",
    "                \"withheld_in_countries\", \"entities\", \"lang\",\n",
    "                \"possibly_sensitive\", \"extended_tweet\"]\n",
    "    df_cen = df_cen[worthKeeping]\n",
    "    \n",
    "    #some tweets have NaN as \"possibly sensitive\"â€¦\n",
    "    df_cen['possibly_sensitive'] = df_cen['possibly_sensitive'].fillna(0.0)\n",
    "    \n",
    "    #recover the full text for truncated tweets\n",
    "    dfRaw = df_cen.values\n",
    "    for line in dfRaw:\n",
    "        if not pd.isna(line[-1]):\n",
    "            line[0] = line[-1][\"full_text\"]   \n",
    "        #remove urls from tweets\n",
    "        #they are shortened anyway so we can't make use of them\n",
    "        line[0] = re.sub(r'http\\S+', '', line[0])\n",
    "        #flatten retweets\n",
    "        line[0] = re.sub(r'RT @\\S+:', '', line[0])\n",
    "    \n",
    "    #remove \"extended_tweet\"\n",
    "    dfRaw = np.delete(dfRaw, len(worthKeeping)-1, axis=1) \n",
    "    worthKeeping.remove(\"extended_tweet\")\n",
    "    \n",
    "    #remove \"truncated\"\n",
    "    dfRaw = np.delete(dfRaw, 1, axis=1) \n",
    "    worthKeeping.remove(\"truncated\")\n",
    "    \n",
    "    #extract hashtags seperately\n",
    "    for line in dfRaw:\n",
    "        line[3] = [x[\"text\"] for x in line[3][\"hashtags\"]]\n",
    "    worthKeeping[3] = \"hashtags\"\n",
    "    \n",
    "    #create a feature for user-verified and user-followers_count\n",
    "    verified = [line[1][\"verified\"] for line in dfRaw]\n",
    "    followers = [line[1][\"followers_count\"] for line in dfRaw]\n",
    "    \n",
    "    #for the location, we keep the country name and discard the rest\n",
    "    location = [findCountry(line[1][\"location\"]) for line in dfRaw]\n",
    "    dfRaw = np.c_[dfRaw, verified, followers, location]\n",
    "    worthKeeping += [\"verified_account\", \"followers_count\", \"location\"]\n",
    "    \n",
    "    #binary feature for whether the tweet has been withheld anywhere\n",
    "    withheld = []\n",
    "    for line in dfRaw:\n",
    "        if not isinstance(line[2], list):\n",
    "            line[2] = []\n",
    "        withheld.append(len(line[2]) != 0)\n",
    "            \n",
    "    dfRaw = np.c_[dfRaw, withheld]\n",
    "    worthKeeping += [\"withheld_anywhere\"]\n",
    "    \n",
    "    #popularity feature:\n",
    "    #build a score based on the values of followers_count, favourites_count, statuses_count\n",
    "    #compute a score from 0 to 1 for each, with (x - min)/(max - min), then comptute the average of these scores \n",
    "    followers_count = np.array([line[1][\"followers_count\"] for line in dfRaw])\n",
    "    favourites_count = np.array([line[1][\"favourites_count\"] for line in dfRaw])\n",
    "    score = (1/2) * (normalize(followers_count) + normalize(favourites_count))\n",
    "    dfRaw = np.c_[dfRaw, score]\n",
    "    worthKeeping += [\"popularity_score\"]\n",
    "    \n",
    "    #reassemble the data in a pandas dataframe and remove the column \"user\"\n",
    "    df_cen = pd.DataFrame(dfRaw, columns = worthKeeping)\n",
    "    cleanCols = filter(lambda x: x != \"user\", worthKeeping)\n",
    "    df_clean = df_cen[cleanCols]\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PREPROCESSING:\n",
    "    df = preprocess_data()\n",
    "    df.to_csv(language_data_path)\n",
    "else: \n",
    "    df = pd.read_csv(language_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725d1da",
   "metadata": {},
   "source": [
    "## Splitting the Data by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_country_dataframes(df): \n",
    "    dataframes_dict = {}\n",
    "    \n",
    "    df_english = df[df['lang'] == \"en\"] \n",
    "    dataframes_dict['English'] = df_english\n",
    "    \n",
    "    df_turkish = df[df['lang'] == \"tr\"] \n",
    "    dataframes_dict['Turkish'] = df_turkish\n",
    "    \n",
    "    df_urdu = df[df['lang'] == \"ur\"]\n",
    "    dataframes_dict['Urdu'] = df_urdu\n",
    "    \n",
    "    df_japanese = df[df['lang'] == \"ja\"] \n",
    "    dataframes_dict['Japanese'] = df_japanese\n",
    "    \n",
    "    df_spanish = df[df['lang'] == \"es\"] \n",
    "    dataframes_dict['Spanish'] = df_spanish\n",
    "    \n",
    "    df_thai = df[df['lang'] == \"th\"] \n",
    "    dataframes_dict['Thai'] = df_thai\n",
    "    \n",
    "    df_portuguese = df[df['lang'] == \"pt\"] \n",
    "    dataframes_dict['Portuguese'] = df_portuguese\n",
    "    \n",
    "    df_arabic = df[df['lang'] == \"ar\"] \n",
    "    dataframes_dict['Arabic'] = df_arabic\n",
    "    \n",
    "    df_indian = df[df['lang'] == \"in\"] \n",
    "    dataframes_dict['Indian'] = df_indian\n",
    "    \n",
    "    return dataframes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2927ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict = make_country_dataframes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PREPROCESSING:\n",
    "    df = dataframes_dict[LANGUAGE_RUN]\n",
    "    df.to_csv(language_data_path)\n",
    "else: \n",
    "    df = pd.read_csv(language_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4827c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4692d23",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34620738",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded = ['possibly_sensitive', 'verified_account',\\\n",
    "                    'followers_count', 'popularity_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "def encode_features(df, concat_all_features=False):\n",
    "    df_features = df.copy()\n",
    "    if FIT_CORPUS_FEATURE_SPACE:\n",
    "        corpus = [sentence if isinstance(sentence, str) else '' for sentence in list(df_features['text'].values)]\n",
    "        \n",
    "        if FEATURE_SPACE == 'BOW':\n",
    "            bow_texts = CountVectorizer().fit_transform(corpus)\n",
    "            with open((feature_space_path + '.pickle'), 'wb') as pkl:\n",
    "                pickle.dump(bow_texts, pkl)\n",
    "            text_vector = bow_texts\n",
    "        \n",
    "        elif FEATURE_SPACE == 'TFIDF':\n",
    "            tfidf_vectors = TfidfVectorizer().fit_transform(corpus) \n",
    "            with open((feature_space_path + '.pickle'), 'wb') as pkl:\n",
    "                pickle.dump(tfidf_vectors, pkl)\n",
    "            text_vector = tfidf_vectors\n",
    "        \n",
    "        else: \n",
    "            model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "            tweets_embeddings = model.encode(sentences=corpus, batch_size=32, \n",
    "                                             show_progress_bar=True, convert_to_numpy=True, \n",
    "                                            normalize_embeddings=True)\n",
    "            \n",
    "            with open(feature_space_path + '.pickle', 'wb') as pkl:\n",
    "                pickle.dump(tweets_embeddings, pkl)\n",
    "            text_vector = tweets_embeddings\n",
    "        \n",
    "    else: \n",
    "        if FEATURE_SPACE == 'BOW':\n",
    "            with open(os.path.join(feature_space_path + '.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl)\n",
    "        elif FEATURE_SPACE == 'TFIDF': \n",
    "            with open(os.path.join(feature_space_path + '.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl)\n",
    "        else: \n",
    "            with open(os.path.join(feature_space_path + '.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl) \n",
    "    \n",
    "    country_label = preprocessing.LabelEncoder()\n",
    "    countries_encoded = country_label.fit_transform(list(df.location.values))\n",
    "    df_features['Country_encoded'] = countries_encoded\n",
    "    \n",
    "    df_features = df_features.astype({\"possibly_sensitive\": float, \"verified_account\": float,\\\n",
    "                                'followers_count':int, 'popularity_score': float})\n",
    "    \n",
    "    y = df_features[\"withheld_anywhere\"].astype(int)\n",
    "    \n",
    "    X = df_features[features_encoded].copy().to_numpy()\n",
    "    \n",
    "    if concat_all_features: \n",
    "        X = np.concatenate((X, text_vector.toarray()), axis=1)\n",
    "        \n",
    "    else:\n",
    "        X = text_vector.toarray()\n",
    "        \n",
    "    X = normalize(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dde667",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = encode_features(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7b4c3",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LogisticRegression(random_state=random_seed)\n",
    "start_time = time.time()\n",
    "y_pred = base_model.fit(X_train, y_train).predict(X_test)\n",
    "end_time = time.time()\n",
    "res = classification_report(y_test, y_pred, target_names=target_names) \n",
    "print(\"Baseline model - Logisitic Regression: \")\n",
    "print(res)\n",
    "print(\"Time to train and Validate the Logisitic Regression Model: \" + str(round(end_time - start_time, 2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'SVM': SVC(random_state=random_seed), \n",
    "          'Random Forest': RandomForestClassifier(random_state=random_seed),\n",
    "          'Gaussian Naive Bayes': GaussianNB()}\n",
    "\n",
    "def run_models(models, save_results=False): \n",
    "    scores = {}\n",
    "    for name in models.keys(): \n",
    "        start_time = time.time()\n",
    "        y_pred = models[name].fit(X_train, y_train).predict(X_test)\n",
    "        end_time = time.time()\n",
    "        res = classification_report(y_test, y_pred, target_names=target_names, output_dict=True, zero_division=0) \n",
    "        scores[name] = res\n",
    "        print(\"Model: \" + name)\n",
    "        print(\"Time to train and Validate the model \" + name +\" : \" + str(round(end_time - start_time, 2)) + \"seconds\")\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names, output_dict=False, zero_division=0))\n",
    "    \n",
    "    if save_results: \n",
    "        with open(results_path, 'w') as f:\n",
    "            for key, value in scores.items(): \n",
    "                f.write('%s:%s\\n' % (key, value))\n",
    "                f.write('\\n')\n",
    "                \n",
    "print(FEATURE_SPACE)\n",
    "run_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5e21f",
   "metadata": {},
   "source": [
    "## Multiple Country Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987292f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dfs_by_country(df, country_list):\n",
    "    df_list = []\n",
    "    for country in country_list: \n",
    "        df_tmp = df[df['location'] == country].copy()\n",
    "        df_list.append(df_tmp)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_run = {'SVM': SVC(random_state=random_seed)}\n",
    "df_countries = df.copy()\n",
    "countries = ['France', 'Turkey', 'Germany', 'India']\n",
    "df_list = make_dfs_by_country(df, countries)  \n",
    "for idx, df in enumerate(df_list): \n",
    "    X, y = encode_features(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print('Country: ' + str(countries[idx]))\n",
    "    run_models(model_to_run, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da1761",
   "metadata": {},
   "source": [
    "## Data Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26980e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_France = df[df['location'] == 'France'].copy()\n",
    "France_counts = list(df_France.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_Turkey = df[df['location'] == 'India'].copy()\n",
    "Turkey_counts = list(df_Turkey.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_Germany = df[df['location'] == 'Germany'].copy()\n",
    "Germany_counts = list(df_Germany.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_India = df[df['location'] == 'India'].copy()\n",
    "India_counts = list(df_India.withheld_anywhere.value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Not Censored', 'Censored']\n",
    "specs = [[{'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}]]\n",
    "fig = make_subplots(2, 2, specs=specs,\n",
    "                    subplot_titles=countries)\n",
    "\n",
    "fig.add_trace(go.Pie(labels=labels, values=France_counts, scalegroup='one',\n",
    "                     name=\"France\"), 1, 1)\n",
    "fig.add_trace(go.Pie(labels=labels, values=Turkey_counts, scalegroup='one',\n",
    "                     name=\"Turkey\"), 1, 2)\n",
    "fig.add_trace(go.Pie(labels=labels, values=Germany_counts, scalegroup='one',\n",
    "                     name=\"Germany\"), 2, 1)\n",
    "fig.add_trace(go.Pie(labels=labels, values=India_counts, scalegroup='one',\n",
    "                     name=\"India\"), 2, 2)\n",
    "\n",
    "\n",
    "fig.update_layout(title_text='Percentage of Censorded Tweets by Countries')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://plotly.com/python/roc-and-pr-curves/\n",
    "def visualize_roc_curve(y_test, y_pred):\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    fig = px.area(\n",
    "        x=false_positive_rate, y=true_positive_rate,\n",
    "        title=f'ROC Curve (AUC={auc(false_positive_rate, true_positive_rate):.4f})',\n",
    "        labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "        width=700, height=500\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=0, y1=1\n",
    "    )\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig.update_xaxes(constrain='domain')\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

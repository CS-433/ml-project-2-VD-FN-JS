{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e38b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: preprocessor in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (1.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: vaderSentiment in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install preprocessor\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7a17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import preprocessor as p\n",
    "import re\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee57dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76d1a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/v82wb7j964v3b7b70x4rrvvr0000gn/T/ipykernel_32587/3730228968.py:2: DtypeWarning: Columns (13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_input = pd.read_csv(csv_path)\n"
     ]
    }
   ],
   "source": [
    "csv_path = 'Output/df_cen.csv'\n",
    "df_input = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9572a",
   "metadata": {},
   "source": [
    " ## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3775bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_countries = ['Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', \\\n",
    "                   'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', \\\n",
    "                   'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', \\\n",
    "                   'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', \\\n",
    "                   'Bhutan', 'Bolivia, Plurinational State of', 'Bonaire, Sint Eustatius and Saba', \\\n",
    "                   'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', \\\n",
    "                   'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', \\\n",
    "                   'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', \\\n",
    "                   'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', \\\n",
    "                   'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', \\\n",
    "                   'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', \"Côte d'Ivoire\", \\\n",
    "                   'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark',\n",
    "                   'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', \\\n",
    "                   'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', \\\n",
    "                   'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', \\\n",
    "                   'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', \\\n",
    "                   'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', \\\n",
    "                   'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', \\\n",
    "                   'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', \\\n",
    "                   'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', \\\n",
    "                   'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', \\\n",
    "                   'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \"Korea, Democratic People's Republic of\", \\\n",
    "                   'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", \\\n",
    "                   'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', \\\n",
    "                   'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', \\\n",
    "                   'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', \\\n",
    "                   'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', \\\n",
    "                   'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', \\\n",
    "                   'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', \\\n",
    "                   'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \\\n",
    "                   'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', \\\n",
    "                   'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', \\\n",
    "                   'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', \\\n",
    "                   'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', \\\n",
    "                   'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia',\\\n",
    "                   'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', \\\n",
    "                   'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', \\\n",
    "                   'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', \\\n",
    "                   'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', \\\n",
    "                   'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', \\\n",
    "                   'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', \\\n",
    "                   'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', \\\n",
    "                   'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', \\\n",
    "                   'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', \\\n",
    "                   'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', \\\n",
    "                   'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', \\\n",
    "                   'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Viet Nam', \\\n",
    "                   'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', \\\n",
    "                   'Zambia', 'Zimbabwe']\n",
    "\n",
    "def find_country(x):\n",
    "        for country in list_of_countries:\n",
    "            if x and country in x:\n",
    "                return country\n",
    "        return None\n",
    "    \n",
    "def normalize(array):\n",
    "        return (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "    \n",
    "def preprocess_df(df_input):\n",
    "    features_worth_keeping = [\"text\", \"truncated\", \"user\",\n",
    "                    \"withheld_in_countries\", \"entities\", \"lang\",\n",
    "                    \"possibly_sensitive\", \"extended_tweet\"]\n",
    "    \n",
    "    df_cen = df_input[features_worth_keeping]\n",
    "    df_cen['possibly_sensitive'] = df_cen['possibly_sensitive'].fillna(0.0)\n",
    "    df_raw = df_cen.values\n",
    "    for line in df_raw:\n",
    "        line[0] = re.sub(r'http\\S+', '', str(line[0])) #remove urls from tweets\n",
    "        line[0] = re.sub(r'RT @\\S+:', '', str(line[0])) #flatten retweets\n",
    "    \n",
    "    df_raw = np.delete(df_raw, len(features_worth_keeping)-1, axis=1) #remove \"extended_tweet\"\n",
    "    df_raw = np.delete(df_raw, 1, axis=1) #remove \"truncated\"\n",
    "    features_worth_keeping.remove(\"extended_tweet\")\n",
    "    features_worth_keeping.remove(\"truncated\")\n",
    "    \n",
    "    for line in df_raw:\n",
    "        print(type(line[3]))\n",
    "        line[3] = [x[\"text\"] for x in line[3][\"hashtags\"]]\n",
    "    features_worth_keeping[3] = \"hashtags\"\n",
    "    \n",
    "    verified = [line[1][\"verified\"] for line in dfRaw]\n",
    "    followers = [line[1][\"followers_count\"] for line in dfRaw]\n",
    "    user_id = [line[1][\"id\"] for line in dfRaw]\n",
    "\n",
    "    \n",
    "    location = [find_country(line[1][\"location\"]) for line in dfRaw]\n",
    "    \n",
    "    df_raw = np.c_[df_raw, verified, followers, location, user_id]\n",
    "    features_worth_keeping += [\"verified_account\", \"followers_count\", \"location\", \"user_id\"]\n",
    "    withheld = []\n",
    "    for line in df_raw:\n",
    "        if not isinstance(line[2], list):\n",
    "            line[2] = []\n",
    "        withheld.append(len(line[2]) != 0)\n",
    "            \n",
    "    df_raw = np.c_[df_raw, withheld]\n",
    "    features_worth_keeping += [\"withheld_anywhere\"]\n",
    "    \n",
    "    sentiment = SentimentIntensityAnalyzer() #we made the assumption that sentiment analysis for this analyzer only works for english\n",
    "    res = np.array([[x for x in sentiment.polarity_scores(line[0]).values()] if line[4] == \"en\" \n",
    "                    else [0.0, 0.0, 0.0, 0.0] for line in df_raw])\n",
    "\n",
    "    df_raw = np.c_[df_raw, res]\n",
    "    features_worth_keeping += [\"neg\", \"neu\", \"pos\", \"compound\"]\n",
    "    \n",
    "    followers_count = np.array([line[1][\"followers_count\"] for line in df_raw])\n",
    "    favourites_count = np.array([line[1][\"favourites_count\"] for line in df_raw])\n",
    "    statuses_count = np.array([line[1][\"statuses_count\"] for line in df_raw])\n",
    "    \n",
    "    score = (1/3) * (normalize(followers_count) + normalize(favourites_count) + normalize(statuses_count))\n",
    "    df_raw = np.c_[df_raw, score]\n",
    "    worthKeeping += [\"popularity_score\"]\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(df_raw, columns = worthKeeping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31c0bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/v82wb7j964v3b7b70x4rrvvr0000gn/T/ipykernel_32587/2307944304.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cen['possibly_sensitive'] = df_cen['possibly_sensitive'].fillna(0.0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpreprocess_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mpreprocess_df\u001b[0;34m(df_input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m df_raw:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(line[\u001b[38;5;241m3\u001b[39m]))\n\u001b[0;32m---> 79\u001b[0m     line[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhashtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     80\u001b[0m features_worth_keeping[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhashtags\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m verified \u001b[38;5;241m=\u001b[39m [line[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverified\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m dfRaw]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "preprocess_df(df_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb731fe0",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aaeea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_CORPUS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded = ['possibly_sensitive', 'verified_account',\\\n",
    "                    'followers_count', 'user_id', 'neg', 'neu', \\\n",
    "                    'pos', 'compound', 'popularity_score', 'text_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b249e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(df):\n",
    "    if FIT_CORPUS:\n",
    "        corpus = [sentence for sentence in list(df['text'].values)]\n",
    "        bow_model = CountVectorizer()\n",
    "        bow_model.fit(corpus)\n",
    "        bow_texts = bow_model.transform(list(df['text'].values))\n",
    "        with open('bow_vectors.pickle', 'wb') as pkl:\n",
    "            pickle.dump(bow_texts, pkl)\n",
    "        \n",
    "        tfidf_model = TfidfVectorizer()\n",
    "        tfidf_model.fit(corpus) \n",
    "        tfdif_text = tfidf_model.transform(list(df_english['text'].values)) \n",
    "        with open('tfidf_vectors.pickle', 'wb') as pkl:\n",
    "            pickle.dump(tfidf_vectors, pkl)\n",
    "        \n",
    "        model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "        tweets_embeddings = model.encode(sentences=all_tweets, batch_size=32, show_progress_bar=True, convert_to_numpy=True)\n",
    "        with open('tweets_embeddings.pickle', 'wb') as pkl:\n",
    "            pickle.dump(tweets_embeddings, pkl)\n",
    "        \n",
    "        \n",
    "    else: \n",
    "        with open('bow_vectors.pickle', 'rb') as pkl:\n",
    "            bow_vectors = pickle.load(pkl)\n",
    "        with open('tfidf_vectors.pickle', 'rb') as pkl:\n",
    "            tfdif_text = pickle.load(pkl)\n",
    "        with open('tweets_embeddings.pickle', 'rb') as pkl:\n",
    "            tweets_embeddings = pickle.load(pkl)\n",
    "            \n",
    "    \n",
    "    df['bow_vectors'] = bow_vectors\n",
    "    df['tfidf_vectors'] = tfdif_text\n",
    "    df['tweets_embeddings'] = tweets_embeddings\n",
    "    \n",
    "    country_label = preprocessing.LabelEncoder()\n",
    "    countries_encoded = country_label.fit_transform(list(df.location.values))\n",
    "    df['Country_encoded'] = countries_encoded\n",
    "    \n",
    "    df = df.astype({\"possibly_sensitive\": float, \"verified_account\": float,\\\n",
    "                                'followers_count':int, 'user_id': int, 'neg': float, 'neu': float, \n",
    "                               'pos': float, 'compound': float, 'popularity_score': float})\n",
    "    \n",
    "    y = df[\"withheld_anywhere\"].astype(int)\n",
    "    X = df[features_encoded].copy().to_numpy()\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = encode_features(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71958f",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "models = {'SVM': SVC(random_state=seed), \n",
    "          'Random Forest': RandomForestClassifier(random_state=seed), \n",
    "          'Gaussian Naive Bayes': GaussianNB()}\n",
    "scores = {}\n",
    "for name in models.keys(): \n",
    "    y_pred = models[name].fit(X_train, y_train).predict(X_test)\n",
    "    target_names = ['False', 'True']\n",
    "    res = classification_report(y_test, y_pred, target_names=target_names) \n",
    "    print(name)\n",
    "    print(res)\n",
    "    scores[name] = res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

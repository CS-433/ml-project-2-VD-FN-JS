{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfa4051",
   "metadata": {},
   "source": [
    "## Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e38b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install preprocessor\n",
    "!pip install vader-multi\n",
    "!pip install torchmetrics\n",
    "!pip install sentence-transformers\n",
    "!pip install gensim\n",
    "!pip install requests\n",
    "!pip install transvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d4932",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7a17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gensim\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import preprocessor as p\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transvec.transformers import TranslationWordVectorizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d7b3e",
   "metadata": {},
   "source": [
    "## Run Flags and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d1a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = date.today()\n",
    "\n",
    "random_seed = 42\n",
    "target_names = ['Not Censord', 'Censord']\n",
    "\n",
    "RUN_PREPROCESSING = False\n",
    "RUN_COUNTRY_DIVISION = False\n",
    "FIT_CORPUS_FEATURE_SPACE = True\n",
    "\n",
    "LANGUAGE_RUN = 'English'\n",
    "FEATURE_SPACE = 'Sentence2Vec' #BOW, TFIDF\n",
    "\n",
    "\n",
    "data_path = 'Data'\n",
    "results_path = 'Results'\n",
    "vector_path = 'vectors'\n",
    "\n",
    "clean_dataframe_path = 'Output/df_clean.csv'\n",
    "english_dataframe_path = 'Output/df_english.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9572a",
   "metadata": {},
   "source": [
    " ## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3775bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfCountries = ['France', 'Turkey', 'Germany', 'India']\n",
    "def findCountry(x): \n",
    "    for country in listOfCountries:\n",
    "        if x and country in x:\n",
    "            return country\n",
    "    return None\n",
    "\n",
    "def normalize(array):\n",
    "        return (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "    \n",
    "def preprocess_data():\n",
    "    #extract the data from the json files\n",
    "    dfs = []\n",
    "    for r, d, f in os.walk('Data/'):\n",
    "        for file in f:\n",
    "            if 'withheldtweets.json' in file or \"plus_one_control.json\" in file:  # alt: if 'control' in file:\n",
    "                dfs.append(pd.read_json('%s/%s' % (r, file), lines=True))\n",
    "    df_cen = pd.concat(dfs)\n",
    "    \n",
    "    #keep only the features that are worth keeping\n",
    "    worthKeeping = [\"text\", \"truncated\", \"user\",\n",
    "                \"withheld_in_countries\", \"entities\", \"lang\",\n",
    "                \"possibly_sensitive\", \"extended_tweet\"]\n",
    "    df_cen = df_cen[worthKeeping]\n",
    "    \n",
    "    #some tweets have NaN as \"possibly sensitive\"…\n",
    "    df_cen['possibly_sensitive'] = df_cen['possibly_sensitive'].fillna(0.0)\n",
    "    \n",
    "    #recover the full text for truncated tweets\n",
    "    dfRaw = df_cen.values\n",
    "    for line in dfRaw:\n",
    "        if not pd.isna(line[-1]):\n",
    "            line[0] = line[-1][\"full_text\"]   \n",
    "        #remove urls from tweets\n",
    "        #they are shortened anyway so we can't make use of them\n",
    "        line[0] = re.sub(r'http\\S+', '', line[0])\n",
    "        #flatten retweets\n",
    "        line[0] = re.sub(r'RT @\\S+:', '', line[0])\n",
    "    \n",
    "    #remove \"extended_tweet\"\n",
    "    dfRaw = np.delete(dfRaw, len(worthKeeping)-1, axis=1) \n",
    "    worthKeeping.remove(\"extended_tweet\")\n",
    "    \n",
    "    #remove \"truncated\"\n",
    "    dfRaw = np.delete(dfRaw, 1, axis=1) \n",
    "    worthKeeping.remove(\"truncated\")\n",
    "    \n",
    "    #extract hashtags seperately\n",
    "    for line in dfRaw:\n",
    "        line[3] = [x[\"text\"] for x in line[3][\"hashtags\"]]\n",
    "    worthKeeping[3] = \"hashtags\"\n",
    "    \n",
    "    #create a feature for user-verified and user-followers_count\n",
    "    verified = [line[1][\"verified\"] for line in dfRaw]\n",
    "    followers = [line[1][\"followers_count\"] for line in dfRaw]\n",
    "    \n",
    "    #for the location, we keep the country name and discard the rest\n",
    "    location = [findCountry(line[1][\"location\"]) for line in dfRaw]\n",
    "    dfRaw = np.c_[dfRaw, verified, followers, location]\n",
    "    worthKeeping += [\"verified_account\", \"followers_count\", \"location\"]\n",
    "    \n",
    "    #binary feature for whether the tweet has been withheld anywhere\n",
    "    withheld = []\n",
    "    for line in dfRaw:\n",
    "        if not isinstance(line[2], list):\n",
    "            line[2] = []\n",
    "        withheld.append(len(line[2]) != 0)\n",
    "            \n",
    "    dfRaw = np.c_[dfRaw, withheld]\n",
    "    worthKeeping += [\"withheld_anywhere\"]\n",
    "    \n",
    "    #popularity feature:\n",
    "    #build a score based on the values of followers_count, favourites_count, statuses_count\n",
    "    #compute a score from 0 to 1 for each, with (x - min)/(max - min), then comptute the average of these scores \n",
    "    followers_count = np.array([line[1][\"followers_count\"] for line in dfRaw])\n",
    "    favourites_count = np.array([line[1][\"favourites_count\"] for line in dfRaw])\n",
    "    statuses_count = np.array([line[1][\"statuses_count\"] for line in dfRaw])\n",
    "    score = (1/3) * (normalize(followers_count) + normalize(favourites_count) + normalize(statuses_count))\n",
    "    dfRaw = np.c_[dfRaw, score]\n",
    "    worthKeeping += [\"popularity_score\"]\n",
    "    #sentiment analysis\n",
    "    #https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/? with VADER\n",
    "    #https://github.com/brunneis/vader-multi, same concept but multilingual\n",
    "    #text gets translated into english and then sentiment analysis is applied to the english text\n",
    "    #takes a LOT of time\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    def doThingsAffi(i, line):\n",
    "        if i % 200 == 0: print(i)\n",
    "        try:\n",
    "            return [x for x in analyzer.polarity_scores(line[0]).values()]\n",
    "        except Exception as e: #known error at about 42400, it's an error in the library\n",
    "            print(e, line)\n",
    "            return [0, 0, 0, 0]\n",
    "            \n",
    "    res = np.array([doThingsAffi(i, line) for i, line in enumerate(dfRaw)])\n",
    "    dfRaw = np.c_[dfRaw, res]\n",
    "    worthKeeping += [\"neg\", \"neu\", \"pos\", \"compound\"]\n",
    "    \n",
    "    #reassemble the data in a pandas dataframe and remove the column \"user\"\n",
    "    df_cen = pd.DataFrame(dfRaw, columns = worthKeeping)\n",
    "    cleanCols = filter(lambda x: x != \"user\", worthKeeping)\n",
    "    df_clean = df_cen[cleanCols]\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n",
      "10000\n",
      "10200\n",
      "10400\n",
      "10600\n",
      "10800\n",
      "11000\n",
      "11200\n",
      "11400\n",
      "11600\n",
      "11800\n",
      "12000\n",
      "12200\n",
      "12400\n",
      "12600\n",
      "12800\n",
      "13000\n",
      "13200\n",
      "13400\n",
      "13600\n",
      "13800\n",
      "14000\n",
      "14200\n",
      "14400\n",
      "14600\n",
      "14800\n",
      "15000\n",
      "15200\n",
      "15400\n",
      "15600\n",
      "15800\n",
      "16000\n",
      "16200\n",
      "16400\n",
      "16600\n",
      "16800\n",
      "17000\n",
      "17200\n",
      "17400\n",
      "17600\n",
      "17800\n",
      "18000\n",
      "18200\n",
      "18400\n",
      "18600\n",
      "18800\n",
      "19000\n",
      "19200\n",
      "19400\n",
      "19600\n",
      "19800\n",
      "20000\n",
      "20200\n",
      "20400\n",
      "20600\n",
      "20800\n",
      "21000\n",
      "21200\n",
      "21400\n",
      "21600\n",
      "21800\n",
      "22000\n",
      "22200\n",
      "22400\n",
      "22600\n",
      "22800\n",
      "23000\n",
      "23200\n",
      "23400\n",
      "23600\n",
      "23800\n",
      "24000\n",
      "24200\n",
      "24400\n",
      "24600\n",
      "24800\n",
      "25000\n",
      "25200\n",
      "25400\n",
      "25600\n",
      "25800\n",
      "26000\n",
      "26200\n",
      "26400\n",
      "26600\n",
      "26800\n",
      "27000\n",
      "27200\n",
      "27400\n",
      "27600\n",
      "27800\n",
      "28000\n",
      "28200\n",
      "28400\n",
      "28600\n",
      "28800\n",
      "29000\n",
      "29200\n",
      "29400\n",
      "29600\n",
      "29800\n",
      "30000\n",
      "30200\n",
      "30400\n",
      "30600\n",
      "30800\n",
      "31000\n",
      "31200\n",
      "31400\n",
      "31600\n",
      "<urlopen error [Errno 54] Connection reset by peer> [' سکندر سلطان چیف الیکشن کمیشن کو پاکستانیو کا پیغام 🤣✌️ '\n",
      " {'id': 1060749380, 'id_str': '1060749380', 'name': 'Jawad Ali Khan', 'screen_name': 'SwatiJawadPTI', 'location': 'Mansehra Kpk,Pakistan', 'url': None, 'description': 'Pakistani By Heart,Muslim By Soul and Human By Nature.Follower Of Nation Hero @ImranKhanPTI,The Only Hope For Pakistan.\\r\\nBig Fan Of Shahid Afridi.#PTIFamily', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 1872, 'friends_count': 4748, 'listed_count': 2, 'favourites_count': 230237, 'statuses_count': 184237, 'created_at': 'Fri Jan 04 15:41:01 +0000 2013', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1367462303269027842/2WtrHw2L_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1367462303269027842/2WtrHw2L_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1060749380/1541324056', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list(['IN']) list([]) 'ur' 0.0 False 1872 None True 0.04134897313037959]\n",
      "31800\n",
      "32000\n",
      "32200\n",
      "32400\n",
      "32600\n",
      "32800\n",
      "33000\n",
      "<urlopen error [Errno 54] Connection reset by peer> [' [ Tarık Burak yazdı ] Halil Şimşek Ağabey Anlatıyor… (Hocaefendi ve Risaleler) '\n",
      " {'id': 997144057452355584, 'id_str': '997144057452355584', 'name': \"Kurtar Allah'ım\", 'screen_name': 'Kurtar_Allahim', 'location': \"Selanik'li ve Bursa'lı\", 'url': None, 'description': 'Hizmet insanını/hareketini sevmiş.Ya hizmet, Yine hizmet,Gayrisi hezimet.Bu yol bir lütuf oldu,yoksa onu biz seçmedik. Seçtirene hamd olsun. 6y3a.', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 1317, 'friends_count': 2845, 'listed_count': 0, 'favourites_count': 3473, 'statuses_count': 39878, 'created_at': 'Thu May 17 15:57:24 +0000 2018', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': '000000', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_link_color': '19CF86', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': '000000', 'profile_text_color': '000000', 'profile_use_background_image': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1341129709472628739/SFEQnWxU_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1341129709472628739/SFEQnWxU_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/997144057452355584/1574453104', 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list(['TR']) list([]) 'tr' 0.0 False 1317 None True 0.0008943897114423469]\n",
      "33200\n",
      "<urlopen error [Errno 54] Connection reset by peer> [' شہدائے افواج پاکستان ہمارا سرمایہ افتخار ۔ جنکی لازوال قربانیوں کی بدولت آج پاکستان  آزاد ملک کی حیثیت سےجاناجاتا ہے\\nبیشک وہ…'\n",
      " {'id': 1259207086875070465, 'id_str': '1259207086875070465', 'name': '🐦', 'screen_name': 'Malik__Pk', 'location': 'Pakistan', 'url': None, 'description': \"Don't let the value of your rules & regulations diminish to reach the destination.🙂\", 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 1607, 'friends_count': 1536, 'listed_count': 0, 'favourites_count': 27917, 'statuses_count': 22108, 'created_at': 'Sat May 09 19:45:07 +0000 2020', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1360110450529165316/YkPKFcS5_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1360110450529165316/YkPKFcS5_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1259207086875070465/1612530929', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list(['IN']) list([]) 'ur' 0.0 False 1607 None True 0.00504317208589475]\n",
      "33400\n",
      "33600\n",
      "33800\n",
      "<urlopen error [Errno 54] Connection reset by peer> [' #ڈٹ_کے_کھڑا_ہے_کپتان\\n\\nImran khan is the only hope for Pakistanis We love &amp;respect you because of your sincerity &amp;true leadershi…'\n",
      " {'id': 849640022206709763, 'id_str': '849640022206709763', 'name': 'خنساء شاہد', 'screen_name': 'Khantastic_PTI', 'location': 'Lahore, Pakistan', 'url': 'http://Facebook.com', 'description': '\\u200f\\u200f\\u200f\\u200fشكريه باكستان❤️', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 947, 'friends_count': 454, 'listed_count': 3, 'favourites_count': 62, 'statuses_count': 10707, 'created_at': 'Wed Apr 05 15:09:01 +0000 2017', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1367756825622417410/GUvbiN7M_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1367756825622417410/GUvbiN7M_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/849640022206709763/1612534750', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list(['IN']) list(['ڈٹ_کے_کھڑا_ہے_کپتان']) 'en' 0.0 False 947 None True\n",
      " 0.00010182229360442251]\n",
      "34000\n",
      "34200\n",
      "34400\n",
      "34600\n",
      "34800\n",
      "35000\n",
      "35200\n",
      "35400\n",
      "35600\n",
      "35800\n",
      "36000\n",
      "36200\n",
      "36400\n",
      "36600\n",
      "36800\n",
      "37000\n",
      "37200\n",
      "37400\n",
      "37600\n",
      "37800\n",
      "38000\n",
      "38200\n",
      "38400\n",
      "38600\n",
      "38800\n",
      "39000\n",
      "39200\n",
      "<urlopen error [Errno 54] Connection reset by peer> [' السلام علیکم\\nجمعہ مبارک\\nدعاؤں كی قبوليت كا دن\\n اے الله ہم تیرا شکرادا کرتے ہیں,اپنےصغیرہ\\n کبیرہ گناہوں کی معافی مانگتےہ…'\n",
      " {'id': 1173250483789664256, 'id_str': '1173250483789664256', 'name': 'عدنان سندھو 💢 ٹیم البدر 💢', 'screen_name': 'AA_MW_', 'location': 'Punjab, Pakistan', 'url': 'https://mobile.twitter.com/search?q=from%3A%40AA_MW_&src=typed_query', 'description': '\\u200f\\u200f\\u200f\\u200f\\u200f\\u200f\\u200f\\u200fبحیثیت مسلمان قادیانی کو کافر کہنا میرا مزہبی حق ہے \\nبحیثیت پاکستانی قادیانی کو کافر کہنا میر آئینی حق ہے \\nاور یہ دونوں حق مجھ سے کوئی نہیں چھین سکتا!', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 24255, 'friends_count': 24595, 'listed_count': 6, 'favourites_count': 64569, 'statuses_count': 93159, 'created_at': 'Sun Sep 15 15:01:41 +0000 2019', 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1363906466092965889/iAYjzlH2_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1363906466092965889/iAYjzlH2_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1173250483789664256/1614958560', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list(['IN']) list([]) 'ur' 0.0 False 24255 None True 0.012399293467722977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39400\n",
      "39600\n",
      "<urlopen error [Errno 54] Connection reset by peer> [' Pakistan PM #ImranKhan to seek vote of confidence from National Assembly \\n@ImranKhanPTI @PTIofficial \\n'\n",
      " {'id': 1236997246299901954, 'id_str': '1236997246299901954', 'name': 'Aamir', 'screen_name': 'aamir_1ak', 'location': 'Timbaktu', 'url': None, 'description': 'Grammarian.Diehard supporter of Imran khan.Retweets are not endorsements.Bleed green....Pakistan Zindabad.', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 806, 'friends_count': 1255, 'listed_count': 0, 'favourites_count': 2840, 'statuses_count': 6681, 'created_at': 'Mon Mar 09 12:48:34 +0000 2020', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1365681678333673474/5ecxLzUF_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1365681678333673474/5ecxLzUF_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1236997246299901954/1583759057', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list(['IN']) list(['ImranKhan']) 'en' 0.0 False 806 None True\n",
      " 0.0005563412235507429]\n",
      "39800\n",
      "40000\n",
      "40200\n",
      "40400\n",
      "40600\n",
      "40800\n",
      "41000\n",
      "41200\n",
      "41400\n",
      "41600\n",
      "41800\n",
      "42000\n",
      "42200\n",
      "42400\n",
      "42600\n",
      "list index out of range ['@taeyreIics Nossa não'\n",
      " {'id': 1065355051978362881, 'id_str': '1065355051978362881', 'name': 'Victor 🐻', 'screen_name': 'lvlyjongin', 'location': 'honeymoon ave. ', 'url': 'http://jongin.xn--kibum-vq2c', 'description': '𖤝 leia o carrd antes de seguir 𖤝 ㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤㅤFan account. Reserva: @zkdlinha', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 2120, 'friends_count': 1897, 'listed_count': 40, 'favourites_count': 21421, 'statuses_count': 63832, 'created_at': 'Wed Nov 21 21:23:32 +0000 2018', 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1358984287266168833/9jaVvmDg_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1358984287266168833/9jaVvmDg_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1065355051978362881/1610922152', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list([]) list([]) 'pt' 0.0 False 2120 None False 0.004194979935348656]\n",
      "list index out of range ['@julia_sayuri13 não'\n",
      " {'id': 1196507236782264320, 'id_str': '1196507236782264320', 'name': 'vic.', 'screen_name': 'narumaru__', 'location': 'Curitiba', 'url': 'https://www.instagram.com/_narumaru', 'description': 'girls/girls/boys.', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 127, 'friends_count': 252, 'listed_count': 0, 'favourites_count': 20901, 'statuses_count': 4951, 'created_at': 'Mon Nov 18 19:15:41 +0000 2019', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1366941362101321728/ajsUGyQA_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1366941362101321728/ajsUGyQA_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1196507236782264320/1614738320', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list([]) list([]) 'und' 0.0 False 127 None False 0.0036759859769750635]\n",
      "42800\n",
      "43000\n",
      "43200\n",
      "43400\n",
      "43600\n",
      "43800\n",
      "44000\n",
      "44200\n",
      "44400\n",
      "44600\n",
      "44800\n",
      "45000\n",
      "list index out of range ['لا'\n",
      " {'id': 1119720377901563904, 'id_str': '1119720377901563904', 'name': '🍌🐉 #정인아_미안해', 'screen_name': 'Marovilx', 'location': None, 'url': 'https://curiouscat.me/Marovilx', 'description': 'Kkeojyeo 💟', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 46, 'friends_count': 37, 'listed_count': 0, 'favourites_count': 203, 'statuses_count': 7018, 'created_at': 'Sat Apr 20 21:51:56 +0000 2019', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1345778917802717185/ZnbEsCIg_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1345778917802717185/ZnbEsCIg_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1119720377901563904/1601683483', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list([]) list([]) 'ar' 0.0 False 46 None False 8.211687351445634e-05]\n",
      "list index out of range ['@Kitune_sub_dayo また誤字ったー🥺\\nないね'\n",
      " {'id': 1362898991923425280, 'id_str': '1362898991923425280', 'name': '風羅@ふう', 'screen_name': 'fura__nico', 'location': None, 'url': None, 'description': '優しくて大好きな相方…【@Kitune_sub_dayo】￤元垢…【@Qz7N9SFAeXKleJJ】￤空リプ…【#風羅の声が聞きたいゾ】￤ペアアイコン…【@stpr_masiro】￤姉…【@hiyoco_Rairakku】￤推し様…【@satoniya_】【@______hii______】', 'translator_type': 'none', 'protected': False, 'verified': False, 'followers_count': 188, 'friends_count': 293, 'listed_count': 32, 'favourites_count': 7558, 'statuses_count': 7376, 'created_at': 'Fri Feb 19 22:57:06 +0000 2021', 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': '', 'profile_background_image_url_https': '', 'profile_background_tile': False, 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1373594879398318081/zuiD_aGt_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1373594879398318081/zuiD_aGt_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1362898991923425280/1613786878', 'default_profile': True, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None}\n",
      " list([]) list([]) 'ja' 0.0 False 188 None False 0.0013688529987874302]\n",
      "45200\n",
      "45400\n",
      "45600\n",
      "45800\n",
      "46000\n",
      "46200\n",
      "46400\n",
      "46600\n",
      "46800\n",
      "47000\n",
      "47200\n",
      "47400\n"
     ]
    }
   ],
   "source": [
    "if RUN_PREPROCESSING:\n",
    "    df = preprocess_data()\n",
    "    df.to_csv(clean_dataframe_path)\n",
    "else: \n",
    "    df = pd.read_csv(clean_dataframe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725d1da",
   "metadata": {},
   "source": [
    "## Splitting the Data by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caad3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_country_dataframes(df): \n",
    "    dataframes_dict = {}\n",
    "    \n",
    "    df_english = df[df['lang'] == \"en\"] \n",
    "    dataframes_dict['English'] = df_english\n",
    "    \n",
    "    df_turkish = df[df['lang'] == \"tr\"] \n",
    "    dataframes_dict['Turkish'] = df_turkish\n",
    "    \n",
    "    df_urdu = df[df['lang'] == \"ur\"]\n",
    "    dataframes_dict['Urdu'] = df_urdu\n",
    "    \n",
    "    df_japanese = df[df['lang'] == \"ja\"] \n",
    "    dataframes_dict['Japanese'] = df_japanese\n",
    "    \n",
    "    df_spanish = df[df['lang'] == \"es\"] \n",
    "    dataframes_dict['Spanish'] = df_spanish\n",
    "    \n",
    "    df_thai = df[df['lang'] == \"th\"] \n",
    "    dataframes_dict['Thai'] = df_thai\n",
    "    \n",
    "    df_portuguese = df[df['lang'] == \"pt\"] \n",
    "    dataframes_dict['Portuguese'] = df_portuguese\n",
    "    \n",
    "    df_arabic = df[df['lang'] == \"ar\"] \n",
    "    dataframes_dict['Arabic'] = df_arabic\n",
    "    \n",
    "    df_indian = df[df['lang'] == \"in\"] \n",
    "    dataframes_dict['Indian'] = df_indian\n",
    "    \n",
    "    return dataframes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2927ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict = make_country_dataframes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66867f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PREPROCESSING:\n",
    "    df_english = dataframes_dict['English']\n",
    "    df_english.to_csv(english_dataframe_path)\n",
    "else: \n",
    "    df_english = pd.read_csv(english_dataframe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4692d23",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34620738",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded = ['possibly_sensitive', 'verified_account',\\\n",
    "                    'followers_count', 'user_id', 'neg', 'neu', \\\n",
    "                    'pos', 'compound', 'popularity_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1eb9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(df, concat_all_features=True):\n",
    "    df_features = df.copy()\n",
    "    if FIT_CORPUS_FEATURE_SPACE:\n",
    "        corpus = [sentence if isinstance(sentence, str) else '' for sentence in list(df_features['text'].values)]\n",
    "        \n",
    "        if FEATURE_SPACE == 'BOW':\n",
    "            bow_texts = CountVectorizer().fit_transform(corpus)\n",
    "            with open(('bow_vectors_' + str(LANGUAGE_RUN) + '.pickle'), 'wb') as pkl:\n",
    "                pickle.dump(bow_texts, pkl)\n",
    "            text_vector = bow_texts\n",
    "        \n",
    "        elif FEATURE_SPACE == 'TFIDF':\n",
    "            tfidf_vectors = TfidfVectorizer().fit_transform(corpus) \n",
    "            with open(('tfidf_vectors_' + str(LANGUAGE_RUN) + '.pickle'), 'wb') as pkl:\n",
    "                pickle.dump(tfidf_vectors, pkl)\n",
    "            text_vector = tfidf_vectors\n",
    "        \n",
    "        else: \n",
    "            model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "            tweets_embeddings = model.encode(sentences=corpus, batch_size=32, \n",
    "                                             show_progress_bar=True, convert_to_numpy=True, \n",
    "                                            normalize_embeddings=True)\n",
    "            with open(('tweets_embeddings_' + str(LANGUAGE_RUN) + '.pickle'), 'wb') as pkl:\n",
    "                pickle.dump(tweets_embeddings, pkl)\n",
    "            text_vector = tweets_embeddings\n",
    "        \n",
    "    else: \n",
    "        if FEATURE_SPACE == 'BOW':\n",
    "            with open(os.path.join(vector_path,'bow_vectors.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl)\n",
    "        elif FEATURE_SPACE == 'TFIDF': \n",
    "            with open(os.path.join(vector_path,'tfidf_vectors.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl)\n",
    "        else: \n",
    "            with open(os.path.join(vector_path,'tweets_embeddings.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl) \n",
    "    \n",
    "    country_label = preprocessing.LabelEncoder()\n",
    "    countries_encoded = country_label.fit_transform(list(df.location.values))\n",
    "    df_features['Country_encoded'] = countries_encoded\n",
    "    \n",
    "    df_features = df_features.astype({\"possibly_sensitive\": float, \"verified_account\": float,\\\n",
    "                                'followers_count':int, 'user_id': int, 'neg': float, 'neu': float, \n",
    "                               'pos': float, 'compound': float, 'popularity_score': float})\n",
    "    \n",
    "    y = df_features[\"withheld_anywhere\"].astype(int)\n",
    "    \n",
    "    X = df_features[features_encoded].copy().to_numpy()\n",
    "    \n",
    "    if concat_all_features: \n",
    "        X = np.concatenate((X, text_vector.toarray()), axis=1)\n",
    "        \n",
    "    else:\n",
    "        X = text_vector.toarray()\n",
    "        \n",
    "    X = noramlize(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0dde667",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = encode_features(df_english)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a68c46c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20664, 24411)\n",
      "(5166, 24411)\n",
      "(20664,)\n",
      "(5166,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7b4c3",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f73e5e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model - Logisitic Regression: \n",
      "{'Not Censord': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2053}, 'Censord': {'precision': 0.6025938830816879, 'recall': 1.0, 'f1-score': 0.7520231912066675, 'support': 3113}, 'accuracy': 0.6025938830816879, 'macro avg': {'precision': 0.30129694154084397, 'recall': 0.5, 'f1-score': 0.37601159560333375, 'support': 5166}, 'weighted avg': {'precision': 0.363119387927467, 'recall': 0.6025938830816879, 'f1-score': 0.4531645749567085, 'support': 5166}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "base_model = LogisticRegression(random_state=random_seed)\n",
    "y_pred = base_model.fit(X_train, y_train).predict(X_test)\n",
    "res = classification_report(y_test, y_pred, target_names=target_names) \n",
    "print(\"Baseline model - Logisitic Regression: \")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff6dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF\n"
     ]
    }
   ],
   "source": [
    "models = {'SVM': SVC(random_state=random_seed), \n",
    "          'Random Forest': RandomForestClassifier(random_state=random_seed)}\n",
    "          'Gaussian Naive Bayes': GaussianNB()}\n",
    "\n",
    "def run_models(models, X_train, X_test, y_train, y_test, save_results=True, visualize_roc_curve=True): \n",
    "    scores = {}\n",
    "    for name in models.keys(): \n",
    "        y_pred = models[name].fit(X_train, y_train).predict(X_test)\n",
    "        res = classification_report(y_test, y_pred, target_names=target_names, output_dict=True, zero_division=0) \n",
    "        scores[name] = res\n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names, output_dict=False, zero_division=0))\n",
    "        if visualize_roc_curve: \n",
    "            false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "            \n",
    "            fig = px.area(\n",
    "                x=fpr, y=tpr,\n",
    "                title=f'ROC Curve (AUC={auc(false_positive_rate, true_positive_rate):.4f})',\n",
    "                labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "                width=700, height=500\n",
    "            )\n",
    "            fig.add_shape(\n",
    "                type='line', line=dict(dash='dash'),\n",
    "                x0=0, x1=1, y0=0, y1=1\n",
    "            )\n",
    "            \n",
    "            fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "            fig.update_xaxes(constrain='domain')\n",
    "            fig.show()\n",
    "    \n",
    "    if save_results: \n",
    "        with open(os.path.join(results_path, ('results_' + str(date_today) + '.txt')), 'w') as f:\n",
    "            for key, value in scores.items(): \n",
    "                f.write('%s:%s\\n' % (key, value))\n",
    "                f.write('\\n')\n",
    "                \n",
    "                \n",
    "print(FEATURE_SPACE)\n",
    "run_models(models, X_train, X_test, y_train, y_test, save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5e21f",
   "metadata": {},
   "source": [
    "## Multiple Country Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987292f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dfs_by_country(df, country_list):\n",
    "    df_list = []\n",
    "    for country in country_list: \n",
    "        df_tmp = df[df['location'] == country].copy()\n",
    "        df_list.append(df_tmp)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_run = {'SVM': SVC(random_state=random_seed)}\n",
    "df_countries = df.copy()\n",
    "countries = ['France', 'Turkey', 'Germany', 'India']\n",
    "df_list = make_dfs_by_country(df, countries)  \n",
    "for idx, df in enumerate(df_list): \n",
    "    X, y = encode_features(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print('Country: ' + str(countries[idx]))\n",
    "    run_models(model_to_run, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da1761",
   "metadata": {},
   "source": [
    "## Data Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26980e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_France = df[df['location'] == 'France'].copy()\n",
    "France_counts = list(df_France.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_Turkey = df[df['location'] == 'India'].copy()\n",
    "Turkey_counts = list(df_Turkey.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_Germany = df[df['location'] == 'Germany'].copy()\n",
    "Germany_counts = list(df_Germany.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_India = df[df['location'] == 'India'].copy()\n",
    "India_counts = list(df_India.withheld_anywhere.value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Not Censored', 'Censored']\n",
    "specs = [[{'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}]]\n",
    "fig = make_subplots(2, 2, specs=specs,\n",
    "                    subplot_titles=countries)\n",
    "\n",
    "fig.add_trace(go.Pie(labels=labels, values=France_counts, scalegroup='one',\n",
    "                     name=\"France\"), 1, 1)\n",
    "fig.add_trace(go.Pie(labels=labels, values=Turkey_counts, scalegroup='one',\n",
    "                     name=\"Turkey\"), 1, 2)\n",
    "fig.add_trace(go.Pie(labels=labels, values=Germany_counts, scalegroup='one',\n",
    "                     name=\"Germany\"), 2, 1)\n",
    "fig.add_trace(go.Pie(labels=labels, values=India_counts, scalegroup='one',\n",
    "                     name=\"India\"), 2, 2)\n",
    "\n",
    "\n",
    "fig.update_layout(title_text='Percentage of Censorded Tweets by Countries')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfa4051",
   "metadata": {},
   "source": [
    "## Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e38b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install preprocessor\n",
    "!pip install vader-multi\n",
    "!pip install torchmetrics\n",
    "!pip install sentence-transformers\n",
    "!pip install gensim\n",
    "!pip install requests\n",
    "!pip install transvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d4932",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7a17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import gensim\n",
    "import pickle\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import preprocessor as p\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transvec.transformers import TranslationWordVectorizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d7b3e",
   "metadata": {},
   "source": [
    "## Run Flags and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "76d1a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = date.today()\n",
    "\n",
    "random_seed = 42\n",
    "target_names = ['Not Censord', 'Censord']\n",
    "\n",
    "RUN_PREPROCESSING = False\n",
    "RUN_COUNTRY_DIVISION = False\n",
    "FIT_CORPUS_FEATURE_SPACE = False\n",
    "\n",
    "LANGUAGE_RUN = 'English' #English, Spanish, Thai, Japanese\n",
    "FEATURE_SPACE = 'Sentence2vec' #BOW, TFIDF, Sentence2vec\n",
    "\n",
    "data_path = 'Data'\n",
    "results_path = 'Results'\n",
    "vector_path = 'Vectors'\n",
    "output_path = 'Output'\n",
    "procssed_data_path = 'Processed'\n",
    "\n",
    "language_run_dir = os.path.join(procssed_data_path, LANGUAGE_RUN)\n",
    "feature_space_dir = os.path.join(vector_path, FEATURE_SPACE)\n",
    "\n",
    "\n",
    "if not os.path.isdir(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    \n",
    "if not os.path.isdir(procssed_data_path):\n",
    "    os.makedirs(procssed_data_path)\n",
    "\n",
    "if not os.path.isdir(language_run_dir):\n",
    "    os.makedirs(language_run_dir)\n",
    "    \n",
    "if not os.path.isdir(results_path):\n",
    "    os.makedirs(results_path)\n",
    "    \n",
    "preprocessed_data_path = os.path.join(output_path, \"csv_processed.csv\")\n",
    "language_data_path = os.path.join(language_run_dir, \"csv_processed.csv\")\n",
    "feature_space_path = os.path.join(feature_space_dir, LANGUAGE_RUN)\n",
    "results_path = os.path.join(results_path, ('results_' + str(date_today) + '.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9572a",
   "metadata": {},
   "source": [
    " ## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3775bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfCountries = ['France', 'Turkey', 'Germany', 'India']\n",
    "def findCountry(x): \n",
    "    for country in listOfCountries:\n",
    "        if x and country in x:\n",
    "            return country\n",
    "    return None\n",
    "\n",
    "def normalize(array):\n",
    "        return (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "    \n",
    "def preprocess_data():\n",
    "    #extract the data from the json files\n",
    "    dfs = []\n",
    "    for r, d, f in os.walk('Data/'):\n",
    "        for file in f:\n",
    "            if 'withheldtweets.json' in file or \"plus_one_control.json\" in file:  # alt: if 'control' in file:\n",
    "                dfs.append(pd.read_json('%s/%s' % (r, file), lines=True))\n",
    "    df_cen = pd.concat(dfs)\n",
    "    \n",
    "    #keep only the features that are worth keeping\n",
    "    worthKeeping = [\"text\", \"truncated\", \"user\",\n",
    "                \"withheld_in_countries\", \"entities\", \"lang\",\n",
    "                \"possibly_sensitive\", \"extended_tweet\"]\n",
    "    df_cen = df_cen[worthKeeping]\n",
    "    \n",
    "    #some tweets have NaN as \"possibly sensitive\"‚Ä¶\n",
    "    df_cen['possibly_sensitive'] = df_cen['possibly_sensitive'].fillna(0.0)\n",
    "    \n",
    "    #recover the full text for truncated tweets\n",
    "    dfRaw = df_cen.values\n",
    "    for line in dfRaw:\n",
    "        if not pd.isna(line[-1]):\n",
    "            line[0] = line[-1][\"full_text\"]   \n",
    "        #remove urls from tweets\n",
    "        #they are shortened anyway so we can't make use of them\n",
    "        line[0] = re.sub(r'http\\S+', '', line[0])\n",
    "        #flatten retweets\n",
    "        line[0] = re.sub(r'RT @\\S+:', '', line[0])\n",
    "    \n",
    "    #remove \"extended_tweet\"\n",
    "    dfRaw = np.delete(dfRaw, len(worthKeeping)-1, axis=1) \n",
    "    worthKeeping.remove(\"extended_tweet\")\n",
    "    \n",
    "    #remove \"truncated\"\n",
    "    dfRaw = np.delete(dfRaw, 1, axis=1) \n",
    "    worthKeeping.remove(\"truncated\")\n",
    "    \n",
    "    #extract hashtags seperately\n",
    "    for line in dfRaw:\n",
    "        line[3] = [x[\"text\"] for x in line[3][\"hashtags\"]]\n",
    "    worthKeeping[3] = \"hashtags\"\n",
    "    \n",
    "    #create a feature for user-verified and user-followers_count\n",
    "    verified = [line[1][\"verified\"] for line in dfRaw]\n",
    "    followers = [line[1][\"followers_count\"] for line in dfRaw]\n",
    "    \n",
    "    #for the location, we keep the country name and discard the rest\n",
    "    location = [findCountry(line[1][\"location\"]) for line in dfRaw]\n",
    "    dfRaw = np.c_[dfRaw, verified, followers, location]\n",
    "    worthKeeping += [\"verified_account\", \"followers_count\", \"location\"]\n",
    "    \n",
    "    #binary feature for whether the tweet has been withheld anywhere\n",
    "    withheld = []\n",
    "    for line in dfRaw:\n",
    "        if not isinstance(line[2], list):\n",
    "            line[2] = []\n",
    "        withheld.append(len(line[2]) != 0)\n",
    "            \n",
    "    dfRaw = np.c_[dfRaw, withheld]\n",
    "    worthKeeping += [\"withheld_anywhere\"]\n",
    "    \n",
    "    #popularity feature:\n",
    "    #build a score based on the values of followers_count, favourites_count, statuses_count\n",
    "    #compute a score from 0 to 1 for each, with (x - min)/(max - min), then comptute the average of these scores \n",
    "    followers_count = np.array([line[1][\"followers_count\"] for line in dfRaw])\n",
    "    favourites_count = np.array([line[1][\"favourites_count\"] for line in dfRaw])\n",
    "    score = (1/2) * (normalize(followers_count) + normalize(favourites_count))\n",
    "    dfRaw = np.c_[dfRaw, score]\n",
    "    worthKeeping += [\"popularity_score\"]\n",
    "    \n",
    "    #reassemble the data in a pandas dataframe and remove the column \"user\"\n",
    "    df_cen = pd.DataFrame(dfRaw, columns = worthKeeping)\n",
    "    cleanCols = filter(lambda x: x != \"user\", worthKeeping)\n",
    "    df_clean = df_cen[cleanCols]\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "31c0bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PREPROCESSING:\n",
    "    df = preprocess_data()\n",
    "    df.to_csv(language_data_path)\n",
    "else: \n",
    "    df = pd.read_csv(language_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725d1da",
   "metadata": {},
   "source": [
    "## Splitting the Data by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "caad3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_country_dataframes(df): \n",
    "    dataframes_dict = {}\n",
    "    \n",
    "    df_english = df[df['lang'] == \"en\"] \n",
    "    dataframes_dict['English'] = df_english\n",
    "    \n",
    "    df_turkish = df[df['lang'] == \"tr\"] \n",
    "    dataframes_dict['Turkish'] = df_turkish\n",
    "    \n",
    "    df_urdu = df[df['lang'] == \"ur\"]\n",
    "    dataframes_dict['Urdu'] = df_urdu\n",
    "    \n",
    "    df_japanese = df[df['lang'] == \"ja\"] \n",
    "    dataframes_dict['Japanese'] = df_japanese\n",
    "    \n",
    "    df_spanish = df[df['lang'] == \"es\"] \n",
    "    dataframes_dict['Spanish'] = df_spanish\n",
    "    \n",
    "    df_thai = df[df['lang'] == \"th\"] \n",
    "    dataframes_dict['Thai'] = df_thai\n",
    "    \n",
    "    df_portuguese = df[df['lang'] == \"pt\"] \n",
    "    dataframes_dict['Portuguese'] = df_portuguese\n",
    "    \n",
    "    df_arabic = df[df['lang'] == \"ar\"] \n",
    "    dataframes_dict['Arabic'] = df_arabic\n",
    "    \n",
    "    df_indian = df[df['lang'] == \"in\"] \n",
    "    dataframes_dict['Indian'] = df_indian\n",
    "    \n",
    "    return dataframes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2927ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dict = make_country_dataframes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3a94949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PREPROCESSING:\n",
    "    df = dataframes_dict[LANGUAGE_RUN]\n",
    "    df.to_csv(language_data_path)\n",
    "else: \n",
    "    df = pd.read_csv(language_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b0d234bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25830, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>verified_account</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>location</th>\n",
       "      <th>withheld_anywhere</th>\n",
       "      <th>popularity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>#Balakot \\nPak Army is our pride ‚ù§Ô∏èüëç</td>\n",
       "      <td>['IN']</td>\n",
       "      <td>['Balakot']</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>Ben Dunk \"If you haven't seen Lahore, you hav...</td>\n",
       "      <td>['IN']</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.029567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>#€ÅŸÖÿßÿ±ÿßÿß€åŸÖÿßŸÜ_ÿØŸÅÿßÿπ_Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ\\n\\nIndia has also be...</td>\n",
       "      <td>['IN']</td>\n",
       "      <td>['€ÅŸÖÿßÿ±ÿßÿß€åŸÖÿßŸÜ_ÿØŸÅÿßÿπ_Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ']</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.019556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>After the great Companion Saad bin Muadh embr...</td>\n",
       "      <td>['RU']</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>After the great Companion Saad bin Muadh embr...</td>\n",
       "      <td>['RU']</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>Paying homage to one of the great pilots in t...</td>\n",
       "      <td>['IN']</td>\n",
       "      <td>['Abhinandan']</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>Paying homage to one of the great pilots in t...</td>\n",
       "      <td>['IN']</td>\n",
       "      <td>['Abhinandan']</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.001171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>STRIP CHAT ‚≠êÔ∏è \\nSTRIP CHAT ‚≠êÔ∏è</td>\n",
       "      <td>['DE']</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>195344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.030799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34</td>\n",
       "      <td>Champions of Democracy are in fact champions ...</td>\n",
       "      <td>['IN']</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35</td>\n",
       "      <td>#SouthWaziristan\\nMilitant commander Noorista...</td>\n",
       "      <td>['IN']</td>\n",
       "      <td>['SouthWaziristan']</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.005672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  \\\n",
       "0           1              #Balakot \\nPak Army is our pride ‚ù§Ô∏èüëç    \n",
       "1          11   Ben Dunk \"If you haven't seen Lahore, you hav...   \n",
       "2          15   #€ÅŸÖÿßÿ±ÿßÿß€åŸÖÿßŸÜ_ÿØŸÅÿßÿπ_Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ\\n\\nIndia has also be...   \n",
       "3          28   After the great Companion Saad bin Muadh embr...   \n",
       "4          29   After the great Companion Saad bin Muadh embr...   \n",
       "5          30   Paying homage to one of the great pilots in t...   \n",
       "6          31   Paying homage to one of the great pilots in t...   \n",
       "7          33                    STRIP CHAT ‚≠êÔ∏è \\nSTRIP CHAT ‚≠êÔ∏è     \n",
       "8          34   Champions of Democracy are in fact champions ...   \n",
       "9          35   #SouthWaziristan\\nMilitant commander Noorista...   \n",
       "\n",
       "  withheld_in_countries                     hashtags lang  possibly_sensitive  \\\n",
       "0                ['IN']                  ['Balakot']   en                 0.0   \n",
       "1                ['IN']                           []   en                 0.0   \n",
       "2                ['IN']  ['€ÅŸÖÿßÿ±ÿßÿß€åŸÖÿßŸÜ_ÿØŸÅÿßÿπ_Ÿæÿß⁄©ÿ≥ÿ™ÿßŸÜ']   en                 0.0   \n",
       "3                ['RU']                           []   en                 0.0   \n",
       "4                ['RU']                           []   en                 0.0   \n",
       "5                ['IN']               ['Abhinandan']   en                 0.0   \n",
       "6                ['IN']               ['Abhinandan']   en                 0.0   \n",
       "7                ['DE']                           []   en                 1.0   \n",
       "8                ['IN']                           []   en                 0.0   \n",
       "9                ['IN']          ['SouthWaziristan']   en                 0.0   \n",
       "\n",
       "   verified_account  followers_count location  withheld_anywhere  \\\n",
       "0             False               93      NaN               True   \n",
       "1             False             3525      NaN               True   \n",
       "2             False             3746      NaN               True   \n",
       "3             False              354      NaN               True   \n",
       "4             False              354      NaN               True   \n",
       "5             False              411      NaN               True   \n",
       "6             False              411      NaN               True   \n",
       "7             False           195344      NaN               True   \n",
       "8             False             1961      NaN               True   \n",
       "9             False             1036      NaN               True   \n",
       "\n",
       "   popularity_score  \n",
       "0          0.000797  \n",
       "1          0.029567  \n",
       "2          0.019556  \n",
       "3          0.002028  \n",
       "4          0.002028  \n",
       "5          0.001171  \n",
       "6          0.001171  \n",
       "7          0.030799  \n",
       "8          0.002089  \n",
       "9          0.005672  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4692d23",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "34620738",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded = ['possibly_sensitive', 'verified_account',\\\n",
    "                    'followers_count', 'popularity_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b1eb9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "def encode_features(df, concat_all_features=False):\n",
    "    df_features = df.copy()\n",
    "    if FIT_CORPUS_FEATURE_SPACE:\n",
    "        corpus = [sentence if isinstance(sentence, str) else '' for sentence in list(df_features['text'].values)]\n",
    "        \n",
    "        if FEATURE_SPACE == 'BOW':\n",
    "            bow_texts = CountVectorizer().fit_transform(corpus)\n",
    "            with open((feature_space_path + '.pickle'), 'wb') as pkl:\n",
    "                pickle.dump(bow_texts, pkl)\n",
    "            text_vector = bow_texts\n",
    "        \n",
    "        elif FEATURE_SPACE == 'TFIDF':\n",
    "            tfidf_vectors = TfidfVectorizer().fit_transform(corpus) \n",
    "            with open((feature_space_path + '.pickle'), 'wb') as pkl:\n",
    "                pickle.dump(tfidf_vectors, pkl)\n",
    "            text_vector = tfidf_vectors\n",
    "        \n",
    "        else: \n",
    "            model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "            tweets_embeddings = model.encode(sentences=corpus, batch_size=32, \n",
    "                                             show_progress_bar=True, convert_to_numpy=True, \n",
    "                                            normalize_embeddings=True)\n",
    "            \n",
    "            with open(feature_space_path + '.pickle', 'wb') as pkl:\n",
    "                pickle.dump(tweets_embeddings, pkl)\n",
    "            text_vector = tweets_embeddings\n",
    "        \n",
    "    else: \n",
    "        if FEATURE_SPACE == 'BOW':\n",
    "            with open(os.path.join(feature_space_path + '.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl)\n",
    "        elif FEATURE_SPACE == 'TFIDF': \n",
    "            with open(os.path.join(feature_space_path + '.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl)\n",
    "        else: \n",
    "            with open(os.path.join(feature_space_path + '.pickle'), 'rb') as pkl:\n",
    "                text_vector = pickle.load(pkl) \n",
    "    \n",
    "    country_label = preprocessing.LabelEncoder()\n",
    "    countries_encoded = country_label.fit_transform(list(df.location.values))\n",
    "    df_features['Country_encoded'] = countries_encoded\n",
    "    \n",
    "    df_features = df_features.astype({\"possibly_sensitive\": float, \"verified_account\": float,\\\n",
    "                                'followers_count':int, 'popularity_score': float})\n",
    "    \n",
    "    y = df_features[\"withheld_anywhere\"].astype(int)\n",
    "    \n",
    "    X = df_features[features_encoded].copy().to_numpy()\n",
    "    \n",
    "    if concat_all_features: \n",
    "        X = np.concatenate((X, text_vector), axis=1)\n",
    "        \n",
    "    else:\n",
    "        X = text_vector\n",
    "        \n",
    "    X = normalize(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b0dde667",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = encode_features(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a68c46c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20664, 512)\n",
      "(5166, 512)\n",
      "(20664,)\n",
      "(5166,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7b4c3",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f73e5e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model - Logisitic Regression: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Censord       0.77      0.77      0.77      2053\n",
      "     Censord       0.85      0.85      0.85      3113\n",
      "\n",
      "    accuracy                           0.82      5166\n",
      "   macro avg       0.81      0.81      0.81      5166\n",
      "weighted avg       0.82      0.82      0.82      5166\n",
      "\n",
      "Time to train and Validate the Logisitic Regression Model: 0.42 seconds\n"
     ]
    }
   ],
   "source": [
    "base_model = LogisticRegression(random_state=random_seed)\n",
    "start_time = time.time()\n",
    "y_pred = base_model.fit(X_train, y_train).predict(X_test)\n",
    "end_time = time.time()\n",
    "res = classification_report(y_test, y_pred, target_names=target_names) \n",
    "print(\"Baseline model - Logisitic Regression: \")\n",
    "print(res)\n",
    "print(\"Time to train and Validate the Logisitic Regression Model: \" + str(round(end_time - start_time, 2)) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff6dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model: SVM\n"
     ]
    }
   ],
   "source": [
    "models = {'SVM': SVC(random_state=random_seed), \n",
    "          'Random Forest': RandomForestClassifier(random_state=random_seed),\n",
    "          'Gaussian Naive Bayes': GaussianNB()}\n",
    "\n",
    "def run_models(models, save_results=False): \n",
    "    scores = {}\n",
    "    for name in models.keys(): \n",
    "        print('Training Model: ' + str(name))\n",
    "        start_time = time.time()\n",
    "        y_pred = models[name].fit(X_train, y_train).predict(X_test)\n",
    "        end_time = time.time()\n",
    "        print(end_time)\n",
    "        res = classification_report(y_test, y_pred, target_names=target_names, output_dict=True, zero_division=0) \n",
    "        scores[name] = res\n",
    "        print(\"Model: \" + name)\n",
    "        print(\"Time to train and Validate the model \" + name +\" : \" + str(round(end_time - start_time, 2)) + \"seconds\")\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names, output_dict=False, zero_division=0))\n",
    "    \n",
    "    if save_results: \n",
    "        with open(results_path, 'w') as f:\n",
    "            for key, value in scores.items(): \n",
    "                f.write('%s:%s\\n' % (key, value))\n",
    "                f.write('\\n')\n",
    "                \n",
    "run_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da1761",
   "metadata": {},
   "source": [
    "## Data Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26980e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_France = df[df['location'] == 'France'].copy()\n",
    "France_counts = list(df_France.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_Turkey = df[df['location'] == 'India'].copy()\n",
    "Turkey_counts = list(df_Turkey.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_Germany = df[df['location'] == 'Germany'].copy()\n",
    "Germany_counts = list(df_Germany.withheld_anywhere.value_counts().values)\n",
    "\n",
    "df_India = df[df['location'] == 'India'].copy()\n",
    "India_counts = list(df_India.withheld_anywhere.value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Not Censored', 'Censored']\n",
    "specs = [[{'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}]]\n",
    "fig = make_subplots(2, 2, specs=specs,\n",
    "                    subplot_titles=countries)\n",
    "\n",
    "fig.add_trace(go.Pie(labels=labels, values=France_counts, scalegroup='one',\n",
    "                     name=\"France\"), 1, 1)\n",
    "fig.add_trace(go.Pie(labels=labels, values=Turkey_counts, scalegroup='one',\n",
    "                     name=\"Turkey\"), 1, 2)\n",
    "fig.add_trace(go.Pie(labels=labels, values=Germany_counts, scalegroup='one',\n",
    "                     name=\"Germany\"), 2, 1)\n",
    "fig.add_trace(go.Pie(labels=labels, values=India_counts, scalegroup='one',\n",
    "                     name=\"India\"), 2, 2)\n",
    "\n",
    "\n",
    "fig.update_layout(title_text='Percentage of Censorded Tweets by Countries')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://plotly.com/python/roc-and-pr-curves/\n",
    "def visualize_roc_curve(y_test, y_pred):\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    fig = px.area(\n",
    "        x=false_positive_rate, y=true_positive_rate,\n",
    "        title=f'ROC Curve (AUC={auc(false_positive_rate, true_positive_rate):.4f})',\n",
    "        labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "        width=700, height=500\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=0, y1=1\n",
    "    )\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig.update_xaxes(constrain='domain')\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

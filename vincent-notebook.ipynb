{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28323d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: preprocessor in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (1.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: vaderSentiment in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchmetrics in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (0.11.0)\n",
      "Requirement already satisfied: packaging in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from torchmetrics) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.8.1 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from torchmetrics) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentence-transformers in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: sentencepiece in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: tqdm in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.1.2)\n",
      "Requirement already satisfied: numpy in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (4.21.3)\n",
      "Requirement already satisfied: scipy in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.7.3)\n",
      "Requirement already satisfied: torchvision in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied: nltk in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (0.10.0)\n",
      "Requirement already satisfied: filelock in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: click in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from torchvision->sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/vincentdandenault/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install preprocessor\n",
    "!pip install vaderSentiment\n",
    "!pip install torchmetrics\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "afc09174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import preprocessor as p\n",
    "import re\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4764fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincentdandenault/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3bfdddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'Data'\n",
    "censored_path = 'censored_tweets'\n",
    "control_path = 'plusone_control_tweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ede9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for r, d, f in os.walk('Data'):\n",
    "    for file in f:\n",
    "        if 'withheldtweets.json' in file or 'plus_one_control.json' in file:  # alt: if â€˜control' in file:\n",
    "            dfs.append(pd.read_json('%s/%s' % (r, file), lines=True))\n",
    "\n",
    "df_cen = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12371fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>lang</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>linked</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>withheld_copyright</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-26 10:54:27+00:00</td>\n",
       "      <td>1365253891839971330</td>\n",
       "      <td>1365253891839971328</td>\n",
       "      <td>CHINE - Depuis le 1er janvier 2021, une loi pe...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>fr</td>\n",
       "      <td>2021-02-26 10:54:27.662</td>\n",
       "      <td>quoted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-26 10:48:35+00:00</td>\n",
       "      <td>1365252415444946945</td>\n",
       "      <td>1365252415444946944</td>\n",
       "      <td>#Balakot \\nPak Army is our pride â¤ï¸ğŸ‘ https://t...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-02-26 10:48:35.662</td>\n",
       "      <td>no</td>\n",
       "      <td>[0, 35]</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>{'media': [{'id': 1365252409015033857, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-26 10:45:51+00:00</td>\n",
       "      <td>1365251727574900738</td>\n",
       "      <td>1365251727574900736</td>\n",
       "      <td>RT @ZaidZamanHamid: Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§Ú©Ø«Ø±ÛŒØª ÛØ¯Ø§ÛŒØª Ù†ÛÛŒÚº...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:45:51.661</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Thu Feb 25 18:59:12 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-26 10:45:51+00:00</td>\n",
       "      <td>1365251727574900738</td>\n",
       "      <td>1365251727574900736</td>\n",
       "      <td>RT @ZaidZamanHamid: Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§Ú©Ø«Ø±ÛŒØª ÛØ¯Ø§ÛŒØª Ù†ÛÛŒÚº...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:45:51.661</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Thu Feb 25 18:59:12 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-26 10:38:57+00:00</td>\n",
       "      <td>1365249991137251328</td>\n",
       "      <td>1365249991137251328</td>\n",
       "      <td>RT @SaniaNishtar: Ø³ÛŒØ¯ Ø§Ø¨Ø±Ø§ÛÛŒÙ… Ú©Ø§ ØªØ¹Ù„Ù‚ Ù‚Ø¨Ø§Ø¦Ù„ÛŒ Ø¶...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:38:57.662</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 05:23:00 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-02-26 10:10:45+00:00</td>\n",
       "      <td>1365242894362279938</td>\n",
       "      <td>1365242894362279936</td>\n",
       "      <td>RT @mosa_abumarzook: ÙÙŠ Ù…Ø«Ù„ ÙØ¬Ø± Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ… Ù‚Ø¨Ù„ ...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ar</td>\n",
       "      <td>2021-02-26 10:10:45.659</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IL]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Thu Feb 25 19:04:40 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-02-26 10:10:45+00:00</td>\n",
       "      <td>1365242894362279938</td>\n",
       "      <td>1365242894362279936</td>\n",
       "      <td>RT @mosa_abumarzook: ÙÙŠ Ù…Ø«Ù„ ÙØ¬Ø± Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ… Ù‚Ø¨Ù„ ...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ar</td>\n",
       "      <td>2021-02-26 10:10:45.659</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IL]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Thu Feb 25 19:04:40 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02-26 10:10:46+00:00</td>\n",
       "      <td>1365242898569134080</td>\n",
       "      <td>1365242898569134080</td>\n",
       "      <td>RT @Saimhun: Mujhe aj bhi batting ki bari ni d...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>hi</td>\n",
       "      <td>2021-02-26 10:10:46.662</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>{'media': [{'id': 1365238103892516864, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 09:51:44 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-02-26 10:10:46+00:00</td>\n",
       "      <td>1365242898569134080</td>\n",
       "      <td>1365242898569134080</td>\n",
       "      <td>RT @Saimhun: Mujhe aj bhi batting ki bari ni d...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>hi</td>\n",
       "      <td>2021-02-26 10:10:46.662</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>{'media': [{'id': 1365238103892516864, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 09:51:44 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-02-26 10:59:24+00:00</td>\n",
       "      <td>1365255137552457730</td>\n",
       "      <td>1365255137552457728</td>\n",
       "      <td>RT @iVeenaKhan: (Ø§Û’ Ù¾ÛŒØºÙ…Ø¨Ø±ï·º! Ù„ÙˆÚ¯ÙˆÚº Ø³Û’) Ú©ÛÛ Ø¯Ùˆ ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:59:24.663</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 05:42:25 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                   id               id_str  \\\n",
       "0 2021-02-26 10:54:27+00:00  1365253891839971330  1365253891839971328   \n",
       "1 2021-02-26 10:48:35+00:00  1365252415444946945  1365252415444946944   \n",
       "2 2021-02-26 10:45:51+00:00  1365251727574900738  1365251727574900736   \n",
       "3 2021-02-26 10:45:51+00:00  1365251727574900738  1365251727574900736   \n",
       "4 2021-02-26 10:38:57+00:00  1365249991137251328  1365249991137251328   \n",
       "5 2021-02-26 10:10:45+00:00  1365242894362279938  1365242894362279936   \n",
       "6 2021-02-26 10:10:45+00:00  1365242894362279938  1365242894362279936   \n",
       "7 2021-02-26 10:10:46+00:00  1365242898569134080  1365242898569134080   \n",
       "8 2021-02-26 10:10:46+00:00  1365242898569134080  1365242898569134080   \n",
       "9 2021-02-26 10:59:24+00:00  1365255137552457730  1365255137552457728   \n",
       "\n",
       "                                                text  \\\n",
       "0  CHINE - Depuis le 1er janvier 2021, une loi pe...   \n",
       "1  #Balakot \\nPak Army is our pride â¤ï¸ğŸ‘ https://t...   \n",
       "2  RT @ZaidZamanHamid: Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§Ú©Ø«Ø±ÛŒØª ÛØ¯Ø§ÛŒØª Ù†ÛÛŒÚº...   \n",
       "3  RT @ZaidZamanHamid: Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§Ú©Ø«Ø±ÛŒØª ÛØ¯Ø§ÛŒØª Ù†ÛÛŒÚº...   \n",
       "4  RT @SaniaNishtar: Ø³ÛŒØ¯ Ø§Ø¨Ø±Ø§ÛÛŒÙ… Ú©Ø§ ØªØ¹Ù„Ù‚ Ù‚Ø¨Ø§Ø¦Ù„ÛŒ Ø¶...   \n",
       "5  RT @mosa_abumarzook: ÙÙŠ Ù…Ø«Ù„ ÙØ¬Ø± Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ… Ù‚Ø¨Ù„ ...   \n",
       "6  RT @mosa_abumarzook: ÙÙŠ Ù…Ø«Ù„ ÙØ¬Ø± Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ… Ù‚Ø¨Ù„ ...   \n",
       "7  RT @Saimhun: Mujhe aj bhi batting ki bari ni d...   \n",
       "8  RT @Saimhun: Mujhe aj bhi batting ki bari ni d...   \n",
       "9  RT @iVeenaKhan: (Ø§Û’ Ù¾ÛŒØºÙ…Ø¨Ø±ï·º! Ù„ÙˆÚ¯ÙˆÚº Ø³Û’) Ú©ÛÛ Ø¯Ùˆ ...   \n",
       "\n",
       "                                              source  truncated  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       True   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "3  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "5  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "6  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "7  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "8  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "9  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "\n",
       "   in_reply_to_status_id  in_reply_to_status_id_str  in_reply_to_user_id  \\\n",
       "0                    NaN                        NaN                  NaN   \n",
       "1                    NaN                        NaN                  NaN   \n",
       "2                    NaN                        NaN                  NaN   \n",
       "3                    NaN                        NaN                  NaN   \n",
       "4                    NaN                        NaN                  NaN   \n",
       "5                    NaN                        NaN                  NaN   \n",
       "6                    NaN                        NaN                  NaN   \n",
       "7                    NaN                        NaN                  NaN   \n",
       "8                    NaN                        NaN                  NaN   \n",
       "9                    NaN                        NaN                  NaN   \n",
       "\n",
       "   in_reply_to_user_id_str  ... filter_level lang            timestamp_ms  \\\n",
       "0                      NaN  ...          low   fr 2021-02-26 10:54:27.662   \n",
       "1                      NaN  ...          low   en 2021-02-26 10:48:35.662   \n",
       "2                      NaN  ...          low   ur 2021-02-26 10:45:51.661   \n",
       "3                      NaN  ...          low   ur 2021-02-26 10:45:51.661   \n",
       "4                      NaN  ...          low   ur 2021-02-26 10:38:57.662   \n",
       "5                      NaN  ...          low   ar 2021-02-26 10:10:45.659   \n",
       "6                      NaN  ...          low   ar 2021-02-26 10:10:45.659   \n",
       "7                      NaN  ...          low   hi 2021-02-26 10:10:46.662   \n",
       "8                      NaN  ...          low   hi 2021-02-26 10:10:46.662   \n",
       "9                      NaN  ...          low   ur 2021-02-26 10:59:24.663   \n",
       "\n",
       "      linked display_text_range  withheld_in_countries  \\\n",
       "0     quoted                NaN                    NaN   \n",
       "1         no            [0, 35]                   [IN]   \n",
       "2         no                NaN                   [IN]   \n",
       "3  retweeted                NaN                   [IN]   \n",
       "4         no                NaN                   [IN]   \n",
       "5         no                NaN                   [IL]   \n",
       "6  retweeted                NaN                   [IL]   \n",
       "7         no                NaN                   [IN]   \n",
       "8  retweeted                NaN                   [IN]   \n",
       "9         no                NaN                   [IN]   \n",
       "\n",
       "                                   extended_entities  possibly_sensitive  \\\n",
       "0                                                NaN                 NaN   \n",
       "1  {'media': [{'id': 1365252409015033857, 'id_str...                 0.0   \n",
       "2                                                NaN                 NaN   \n",
       "3                                                NaN                 NaN   \n",
       "4                                                NaN                 NaN   \n",
       "5                                                NaN                 NaN   \n",
       "6                                                NaN                 NaN   \n",
       "7  {'media': [{'id': 1365238103892516864, 'id_str...                 0.0   \n",
       "8  {'media': [{'id': 1365238103892516864, 'id_str...                 0.0   \n",
       "9                                                NaN                 NaN   \n",
       "\n",
       "                                    retweeted_status withheld_copyright  \n",
       "0                                                NaN                NaN  \n",
       "1                                                NaN                NaN  \n",
       "2  {'created_at': 'Thu Feb 25 18:59:12 +0000 2021...                NaN  \n",
       "3  {'created_at': 'Thu Feb 25 18:59:12 +0000 2021...                NaN  \n",
       "4  {'created_at': 'Fri Feb 26 05:23:00 +0000 2021...                NaN  \n",
       "5  {'created_at': 'Thu Feb 25 19:04:40 +0000 2021...                NaN  \n",
       "6  {'created_at': 'Thu Feb 25 19:04:40 +0000 2021...                NaN  \n",
       "7  {'created_at': 'Fri Feb 26 09:51:44 +0000 2021...                NaN  \n",
       "8  {'created_at': 'Fri Feb 26 09:51:44 +0000 2021...                NaN  \n",
       "9  {'created_at': 'Fri Feb 26 05:42:25 +0000 2021...                NaN  \n",
       "\n",
       "[10 rows x 39 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cen.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a9fad",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6939f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = ['id','text', 'truncated', 'user', 'user-location', 'user-verified', 'user-followers_count', \\\n",
    "                    'withheld_in_countries', 'entities', 'lang', 'extended_entities', \\\n",
    "                   'possibly_sensitive', 'extended_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba8b21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "worthKeeping = [\"text\", \"truncated\", \"user\",\n",
    "                \"withheld_in_countries\", \"entities\", \"lang\",\n",
    "                \"possibly_sensitive\", \"extended_tweet\"]\n",
    "df_cen = df_cen[worthKeeping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a9ed824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cen['possibly_sensitive'] = df_cen['possibly_sensitive'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "714e4281",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRaw = df_cen.values\n",
    "for line in dfRaw:\n",
    "    #if not pd.isna(line[-1]):\n",
    "        #line[0] = line[-1][\"full_text\"]\n",
    "        \n",
    "    #remove urls from tweets\n",
    "    #they are shortened anyway so we can't make use of them\n",
    "    line[0] = re.sub(r'http\\S+', '', str(line[0]))\n",
    "    \n",
    "    #flatten retweets\n",
    "    line[0] = re.sub(r'RT @\\S+:', '', str(line[0]))\n",
    "\n",
    "dfRaw = np.delete(dfRaw, len(worthKeeping)-1, axis=1) #remove \"extended_tweet\"\n",
    "worthKeeping.remove(\"extended_tweet\")\n",
    "\n",
    "dfRaw = np.delete(dfRaw, 1, axis=1) #remove \"truncated\"\n",
    "worthKeeping.remove(\"truncated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d1a586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in dfRaw:\n",
    "    line[3] = [x[\"text\"] for x in line[3][\"hashtags\"]]\n",
    "worthKeeping[3] = \"hashtags\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6f526a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a feature for user-verified and user-followers_count\n",
    "verified = [line[1][\"verified\"] for line in dfRaw]\n",
    "followers = [line[1][\"followers_count\"] for line in dfRaw]\n",
    "user_id = [line[1][\"id\"] for line in dfRaw]\n",
    "\n",
    "#for the location, Rebekah suggested to only spot the country name and discard the rest\n",
    "listOfCountries = ['Afghanistan', 'Aland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia, Plurinational State of', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', \"CÃ´te d'Ivoire\", 'Croatia', 'Cuba', 'CuraÃ§ao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', \"Korea, Democratic People's Republic of\", 'Korea, Republic of', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia, Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'RÃ©union', 'Romania', 'Russian Federation', 'Rwanda', 'Saint BarthÃ©lemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'South Sudan', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan, Province of China', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Viet Nam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Yemen', 'Zambia', 'Zimbabwe']\n",
    "def findCountry(x):\n",
    "    for country in listOfCountries:\n",
    "        if x and country in x:\n",
    "            return country\n",
    "    return None\n",
    "\n",
    "location = [findCountry(line[1][\"location\"]) for line in dfRaw]\n",
    "\n",
    "dfRaw = np.c_[dfRaw, verified, followers, location, user_id]\n",
    "worthKeeping += [\"verified_account\", \"followers_count\", \"location\", \"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "163abaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "withheld = []\n",
    "for line in dfRaw:\n",
    "    if not isinstance(line[2], list):\n",
    "        line[2] = []\n",
    "    withheld.append(len(line[2]) != 0)\n",
    "        \n",
    "dfRaw = np.c_[dfRaw, withheld]\n",
    "worthKeeping += [\"withheld_anywhere\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c77b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "#we made the assumption that sentiment analysis for this analyzer only works for english\n",
    "res = np.array([[x for x in sentiment.polarity_scores(line[0]).values()] if line[4] == \"en\" else [0.0, 0.0, 0.0, 0.0] for line in dfRaw])\n",
    "\n",
    "dfRaw = np.c_[dfRaw, res]\n",
    "worthKeeping += [\"neg\", \"neu\", \"pos\", \"compound\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "805e2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#popularity feature:\n",
    "#build a score based on the values of followers_count, favourites_count, statuses_count\n",
    "#compute a score from 0 to 1 for each, with (x - min)/(max - min), then comptute the average of these scores \n",
    "\n",
    "followers_count = np.array([line[1][\"followers_count\"] for line in dfRaw])\n",
    "favourites_count = np.array([line[1][\"favourites_count\"] for line in dfRaw])\n",
    "statuses_count = np.array([line[1][\"statuses_count\"] for line in dfRaw])\n",
    "\n",
    "def normalize(array):\n",
    "    return (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "\n",
    "score = (1/3) * (normalize(followers_count) + normalize(favourites_count) + normalize(statuses_count))\n",
    "dfRaw = np.c_[dfRaw, score]\n",
    "worthKeeping += [\"popularity_score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fee7ff20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>verified_account</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>location</th>\n",
       "      <th>user_id</th>\n",
       "      <th>withheld_anywhere</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>popularity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHINE - Depuis le 1er janvier 2021, une loi pe...</td>\n",
       "      <td>{'id': 1184106601122156544, 'id_str': '1184106...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>fr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>529</td>\n",
       "      <td>None</td>\n",
       "      <td>1184106601122156544</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Balakot \\nPak Army is our pride â¤ï¸ğŸ‘</td>\n",
       "      <td>{'id': 966615015716458497, 'id_str': '96661501...</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>[Balakot]</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>93</td>\n",
       "      <td>None</td>\n",
       "      <td>966615015716458497</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.000542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§Ú©Ø«Ø±ÛŒØª ÛØ¯Ø§ÛŒØª Ù†ÛÛŒÚº Ù„ÛŒÙ†Ø§ Ú†Ø§Û Ø±ÛÛŒÛ” \\nÛÙ…...</td>\n",
       "      <td>{'id': 407984569, 'id_str': '407984569', 'name...</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ur</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>753</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>407984569</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§Ú©Ø«Ø±ÛŒØª ÛØ¯Ø§ÛŒØª Ù†ÛÛŒÚº Ù„ÛŒÙ†Ø§ Ú†Ø§Û Ø±ÛÛŒÛ” \\nÛÙ…...</td>\n",
       "      <td>{'id': 407984569, 'id_str': '407984569', 'name...</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ur</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>753</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>407984569</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø³ÛŒØ¯ Ø§Ø¨Ø±Ø§ÛÛŒÙ… Ú©Ø§ ØªØ¹Ù„Ù‚ Ù‚Ø¨Ø§Ø¦Ù„ÛŒ Ø¶Ù„Ø¹ Ú©Ø±Ù… Ø³Û’ ÛÛ’Û” Ø§Ø­Ø³...</td>\n",
       "      <td>{'id': 1022545022447759360, 'id_str': '1022545...</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ur</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3526</td>\n",
       "      <td>None</td>\n",
       "      <td>1022545022447759360</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70883</th>\n",
       "      <td>à¸–à¹‰à¸²à¹„à¸”à¹‰à¸£à¸±à¸à¹ƒà¸„à¸£à¸ªà¸±à¸à¸„à¸™à¸‚à¸¶à¹‰à¸™à¸¡à¸²à¸ˆà¸£à¸´à¸‡à¹† à¸‰à¸±à¸™à¸„à¸‡à¹‚à¸­à¸šà¸à¸­à¸”à¸—à¸¸à¸à¸‚à¹‰...</td>\n",
       "      <td>{'id': 733700890129883136, 'id_str': '73370089...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>th</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>110</td>\n",
       "      <td>None</td>\n",
       "      <td>733700890129883136</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70884</th>\n",
       "      <td>ãƒªãƒ³ã‚¯ã®å¦–ç²¾ã€€ãƒ´ã‚£ã‚¯ãƒˆãƒ«ãƒ»ãƒ‹ã‚­ãƒ•ã‚©ãƒ­ãƒ•ï¼†\\nãŠèŠ±ç•‘ã®ã‚·ãƒ³ãƒ‡ãƒ¬ãƒ©ã€€ã‚¯ãƒªã‚¹ãƒˆãƒ•ãƒ»ã‚¸ãƒ£ã‚³ãƒ¡ãƒƒãƒ†ã‚£...</td>\n",
       "      <td>{'id': 827750070724354048, 'id_str': '82775007...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ã‚¯ãƒªã‚¹ãƒˆãƒ•ãƒ»ã‚¸ãƒ£ã‚³ãƒ¡ãƒƒãƒ†ã‚£ç”Ÿèª•ç¥­2021, ã‚¯ãƒªã‚¹ãƒˆãƒ•ãƒ»ã‚¸ãƒ£ã‚³ãƒ¡ãƒƒãƒ†ã‚£èª•ç”Ÿç¥­2021, ã‚¯...</td>\n",
       "      <td>ja</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>None</td>\n",
       "      <td>827750070724354048</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70885</th>\n",
       "      <td></td>\n",
       "      <td>{'id': 1352109785383116805, 'id_str': '1352109...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>und</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>None</td>\n",
       "      <td>1352109785383116805</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70886</th>\n",
       "      <td>AÄLIYORUM Ã–ÄLE ARAMIZI 40 DAKÅKA YAPMIÅLAR AÄL...</td>\n",
       "      <td>{'id': 1322266379203022848, 'id_str': '1322266...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>tr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>81</td>\n",
       "      <td>None</td>\n",
       "      <td>1322266379203022848</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70887</th>\n",
       "      <td>The order.                         The creation</td>\n",
       "      <td>{'id': 4503699563, 'id_str': '4503699563', 'na...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>340</td>\n",
       "      <td>None</td>\n",
       "      <td>4503699563</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.002595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70888 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      CHINE - Depuis le 1er janvier 2021, une loi pe...   \n",
       "1                  #Balakot \\nPak Army is our pride â¤ï¸ğŸ‘    \n",
       "2       Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§Ú©Ø«Ø±ÛŒØª ÛØ¯Ø§ÛŒØª Ù†ÛÛŒÚº Ù„ÛŒÙ†Ø§ Ú†Ø§Û Ø±ÛÛŒÛ” \\nÛÙ…...   \n",
       "3       Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§Ú©Ø«Ø±ÛŒØª ÛØ¯Ø§ÛŒØª Ù†ÛÛŒÚº Ù„ÛŒÙ†Ø§ Ú†Ø§Û Ø±ÛÛŒÛ” \\nÛÙ…...   \n",
       "4       Ø³ÛŒØ¯ Ø§Ø¨Ø±Ø§ÛÛŒÙ… Ú©Ø§ ØªØ¹Ù„Ù‚ Ù‚Ø¨Ø§Ø¦Ù„ÛŒ Ø¶Ù„Ø¹ Ú©Ø±Ù… Ø³Û’ ÛÛ’Û” Ø§Ø­Ø³...   \n",
       "...                                                  ...   \n",
       "70883   à¸–à¹‰à¸²à¹„à¸”à¹‰à¸£à¸±à¸à¹ƒà¸„à¸£à¸ªà¸±à¸à¸„à¸™à¸‚à¸¶à¹‰à¸™à¸¡à¸²à¸ˆà¸£à¸´à¸‡à¹† à¸‰à¸±à¸™à¸„à¸‡à¹‚à¸­à¸šà¸à¸­à¸”à¸—à¸¸à¸à¸‚à¹‰...   \n",
       "70884   ãƒªãƒ³ã‚¯ã®å¦–ç²¾ã€€ãƒ´ã‚£ã‚¯ãƒˆãƒ«ãƒ»ãƒ‹ã‚­ãƒ•ã‚©ãƒ­ãƒ•ï¼†\\nãŠèŠ±ç•‘ã®ã‚·ãƒ³ãƒ‡ãƒ¬ãƒ©ã€€ã‚¯ãƒªã‚¹ãƒˆãƒ•ãƒ»ã‚¸ãƒ£ã‚³ãƒ¡ãƒƒãƒ†ã‚£...   \n",
       "70885                                                      \n",
       "70886  AÄLIYORUM Ã–ÄLE ARAMIZI 40 DAKÅKA YAPMIÅLAR AÄL...   \n",
       "70887   The order.                         The creation    \n",
       "\n",
       "                                                    user  \\\n",
       "0      {'id': 1184106601122156544, 'id_str': '1184106...   \n",
       "1      {'id': 966615015716458497, 'id_str': '96661501...   \n",
       "2      {'id': 407984569, 'id_str': '407984569', 'name...   \n",
       "3      {'id': 407984569, 'id_str': '407984569', 'name...   \n",
       "4      {'id': 1022545022447759360, 'id_str': '1022545...   \n",
       "...                                                  ...   \n",
       "70883  {'id': 733700890129883136, 'id_str': '73370089...   \n",
       "70884  {'id': 827750070724354048, 'id_str': '82775007...   \n",
       "70885  {'id': 1352109785383116805, 'id_str': '1352109...   \n",
       "70886  {'id': 1322266379203022848, 'id_str': '1322266...   \n",
       "70887  {'id': 4503699563, 'id_str': '4503699563', 'na...   \n",
       "\n",
       "      withheld_in_countries  \\\n",
       "0                        []   \n",
       "1                      [IN]   \n",
       "2                      [IN]   \n",
       "3                      [IN]   \n",
       "4                      [IN]   \n",
       "...                     ...   \n",
       "70883                    []   \n",
       "70884                    []   \n",
       "70885                    []   \n",
       "70886                    []   \n",
       "70887                    []   \n",
       "\n",
       "                                                hashtags lang  \\\n",
       "0                                                     []   fr   \n",
       "1                                              [Balakot]   en   \n",
       "2                                                     []   ur   \n",
       "3                                                     []   ur   \n",
       "4                                                     []   ur   \n",
       "...                                                  ...  ...   \n",
       "70883                                                 []   th   \n",
       "70884  [ã‚¯ãƒªã‚¹ãƒˆãƒ•ãƒ»ã‚¸ãƒ£ã‚³ãƒ¡ãƒƒãƒ†ã‚£ç”Ÿèª•ç¥­2021, ã‚¯ãƒªã‚¹ãƒˆãƒ•ãƒ»ã‚¸ãƒ£ã‚³ãƒ¡ãƒƒãƒ†ã‚£èª•ç”Ÿç¥­2021, ã‚¯...   ja   \n",
       "70885                                                 []  und   \n",
       "70886                                                 []   tr   \n",
       "70887                                                 []   en   \n",
       "\n",
       "      possibly_sensitive verified_account followers_count  location  \\\n",
       "0                    0.0            False             529      None   \n",
       "1                    0.0            False              93      None   \n",
       "2                    0.0            False             753  Pakistan   \n",
       "3                    0.0            False             753  Pakistan   \n",
       "4                    0.0            False            3526      None   \n",
       "...                  ...              ...             ...       ...   \n",
       "70883                0.0            False             110      None   \n",
       "70884                0.0            False              29      None   \n",
       "70885                0.0            False              28      None   \n",
       "70886                0.0            False              81      None   \n",
       "70887                1.0            False             340      None   \n",
       "\n",
       "                   user_id withheld_anywhere  neg    neu    pos compound  \\\n",
       "0      1184106601122156544             False  0.0    0.0    0.0      0.0   \n",
       "1       966615015716458497              True  0.0  0.789  0.211     0.34   \n",
       "2                407984569              True  0.0    0.0    0.0      0.0   \n",
       "3                407984569              True  0.0    0.0    0.0      0.0   \n",
       "4      1022545022447759360              True  0.0    0.0    0.0      0.0   \n",
       "...                    ...               ...  ...    ...    ...      ...   \n",
       "70883   733700890129883136             False  0.0    0.0    0.0      0.0   \n",
       "70884   827750070724354048             False  0.0    0.0    0.0      0.0   \n",
       "70885  1352109785383116805             False  0.0    0.0    0.0      0.0   \n",
       "70886  1322266379203022848             False  0.0    0.0    0.0      0.0   \n",
       "70887           4503699563             False  0.0  0.588  0.412   0.2732   \n",
       "\n",
       "      popularity_score  \n",
       "0             0.000075  \n",
       "1             0.000542  \n",
       "2             0.007183  \n",
       "3             0.007183  \n",
       "4             0.020478  \n",
       "...                ...  \n",
       "70883         0.002502  \n",
       "70884         0.012251  \n",
       "70885         0.000358  \n",
       "70886         0.000347  \n",
       "70887         0.002595  \n",
       "\n",
       "[70888 rows x 16 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reassemble the data in a pandas dataframe\n",
    "df_cen = pd.DataFrame(dfRaw, columns = worthKeeping)\n",
    "df_cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb64a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanCols = filter(lambda x: x != \"user\", worthKeeping)\n",
    "df_clean = df_cen[cleanCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cb895a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.NUMBER, p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.HASHTAG)\n",
    "df_processed = df_clean.copy()\n",
    "df_processed[\"text\"] = df_clean.apply({\"text\": lambda line: p.clean(line)}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0100a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noduplicates = df_processed.copy()\n",
    "df_noduplicates['withheld_in_countries'] = df_processed['withheld_in_countries'].astype(str)\n",
    "df_noduplicates['hashtags'] = df_processed['hashtags'].astype(str)\n",
    "\n",
    "df_noduplicates.drop_duplicates()\n",
    "cleanCols = filter(lambda x: x != \"user\", worthKeeping)\n",
    "df_clean = df_noduplicates[cleanCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9f51346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_english = df_noduplicates[df_noduplicates['lang'] == \"en\"] \n",
    "#df_turkish = df_noduplicates[df_noduplicates['lang'] == \"tr\"] \n",
    "#df_urdu = df_noduplicates[df_noduplicates['lang'] == \"ur\"]\n",
    "#df_japanese = df_noduplicates[df_noduplicates['lang'] == \"ja\"] \n",
    "#df_spanish = df_noduplicates[df_noduplicates['lang'] == \"es\"] \n",
    "#df_thai = df_noduplicates[df_noduplicates['lang'] == \"th\"] \n",
    "#df_portuguese = df_noduplicates[df_noduplicates['lang'] == \"pt\"] \n",
    "#df_arabic = df_noduplicates[df_noduplicates['lang'] == \"ar\"] \n",
    "#df_indian = df_noduplicates[df_noduplicates['lang'] == \"in\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53c247a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25830, 15)\n"
     ]
    }
   ],
   "source": [
    "df_english = df_clean[df_noduplicates['lang'] == \"en\"] \n",
    "print(df_english.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5546795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_CSV = False\n",
    "if TO_CSV:\n",
    "    df_english.to_csv('df_english_clean', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628207d",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6b400a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = CountVectorizer(stop_words='english')\n",
    "#vectorizer.fit(flat_list_text_tokenized)\n",
    "#def bow_tranform_wrapper(sentences): \n",
    "#    if isinstance(sentences, list): \n",
    "#        return vectorizer.transform(sentences)\n",
    "#    else:\n",
    "#        print('Not a list! Ingore message: ' + str(sentences))\n",
    "#        return vectorizer.transform([sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f13b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/v82wb7j964v3b7b70x4rrvvr0000gn/T/ipykernel_33989/2991236561.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english[\"withheld_anywhere\"] = df_english[\"withheld_anywhere\"].astype(int)\n"
     ]
    }
   ],
   "source": [
    "df_english[\"withheld_anywhere\"] = df_english[\"withheld_anywhere\"].astype(int)\n",
    "y = df_english[\"withheld_anywhere\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15c1f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def word_tokenize_wrapper(sentence): \n",
    "    if isinstance(sentence, str): \n",
    "        sentence = \" \".join(sentences)\n",
    "        return nltk.sent_tokenize(sentence)\n",
    "    else:\n",
    "        print('Not a string! Ingore message: ' + str(sentence))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5f0f2b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_m/v82wb7j964v3b7b70x4rrvvr0000gn/T/ipykernel_33989/687327608.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english['Country_encoded'] = countries_encoded\n"
     ]
    }
   ],
   "source": [
    "country_label = preprocessing.LabelEncoder()\n",
    "countries_encoded = country_label.fit_transform(list(df_english.location.values))\n",
    "df_english['Country_encoded'] = countries_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "644940fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([121, 121, 116, ..., 121, 121, 121])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english['Country_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d5a4ebf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                      object\n",
       "withheld_in_countries     object\n",
       "hashtags                  object\n",
       "lang                      object\n",
       "possibly_sensitive       float64\n",
       "verified_account            bool\n",
       "followers_count           object\n",
       "location                  object\n",
       "user_id                   object\n",
       "withheld_anywhere          int64\n",
       "neg                       object\n",
       "neu                       object\n",
       "pos                       object\n",
       "compound                  object\n",
       "popularity_score          object\n",
       "text_tokenized_list       object\n",
       "Country_encoded            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c35d3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenized_list = list(df_english.apply(lambda x: word_tokenize_wrapper(x['text']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cd3cbfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                      object\n",
       "withheld_in_countries     object\n",
       "hashtags                  object\n",
       "lang                      object\n",
       "possibly_sensitive       float64\n",
       "verified_account         float64\n",
       "followers_count            int64\n",
       "location                  object\n",
       "user_id                    int64\n",
       "withheld_anywhere          int64\n",
       "neg                      float64\n",
       "neu                      float64\n",
       "pos                      float64\n",
       "compound                 float64\n",
       "popularity_score         float64\n",
       "text_tokenized_list       object\n",
       "Country_encoded            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dfb91764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = df_english.astype({\"possibly_sensitive\": float, \"verified_account\": float,\\\n",
    "                                'followers_count':int, 'user_id': int, 'neg': float, 'neu': float, \n",
    "                               'pos': float, 'compound': float, 'popularity_score': float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "83d8a15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24723a6c82644fac8b0acbc3ea2878a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93da3d8066d4281a93a496f869c0848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936b3f7b03024db6b6d33a2e78b3bbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d829adb0f5467bbbc985237c60eda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf35aeacfe9949d4a992dece21162f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbe105a059546068bd1209aa4cf02a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa1be616f8249ea9c57fcca30504b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4e39801efe4aa28ed3018cfccc461e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4459784632546acb27e785d218dbaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f56265b58dd44bc91d8ab31a53e87b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381b5097560c4b38925445b88031567a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf457740f4414932a56e6bf0e46682ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d8c634789449ec88d332f0f08f6256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [129]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df_english[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_tokenized_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m text_tokenized_list\n\u001b[1;32m      2\u001b[0m sbert_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-nli-mean-tokens\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m sentence_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43msbert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_tokenized_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_english[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sentence_embeddings\n\u001b[1;32m      6\u001b[0m df_english[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpossibly_sensitive\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df_english\u001b[38;5;241m.\u001b[39mpossibly_sensitive)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1009\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1011\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1012\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1013\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1017\u001b[0m )\n\u001b[0;32m-> 1018\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1031\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    598\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    600\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:535\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    532\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    533\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 535\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/pytorch_utils.py:243\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:548\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    547\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 548\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:460\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 460\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_english['text_tokenized_list'] = text_tokenized_list\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "sentence_embeddings = sbert_model.encode(text_tokenized_list)\n",
    "df_english['text_embeddings'] = sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_encoded = ['text_embeddings', 'possibly_sensitive', 'verified_account', 'Country_encoded ',\\\n",
    "                    'followers_count', 'user_id', 'neg', 'neu', 'pos', 'compound', 'popularity_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_english[features_encoded].copy().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dac2e5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mcleaned_sentences\u001b[49m, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad62f421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20664,)\n",
      "(5166,)\n",
      "(20664,)\n",
      "(20664,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e718175",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'csr_matrix'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mkeys(): \n\u001b[0;32m----> 8\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      9\u001b[0m     scores[name] \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true, y_pred)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:173\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    171\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    184\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    185\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    186\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "svm = SVC(random_state=seed)\n",
    "random_forest = RandomForestClassifier(random_state=seed)\n",
    "gnb = GaussianNB()\n",
    "models = {'SVM': svm, 'Random Forest': random_forest, 'Gaussian Naive Bayes': gnb}\n",
    "scores = {}\n",
    "for name in models.keys(): \n",
    "    y_pred = models[name].fit(X_train, y_train).predict(X_test)\n",
    "    scores[name] = confusion_matrix(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
